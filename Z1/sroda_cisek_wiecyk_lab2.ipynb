{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m.read_csv(\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdata.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     delimiter=\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "df = pd.read_csv(\n",
    "    \"data.csv\",\n",
    "    delimiter=\",\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"instant\", \"dteday\", \"yr\", \"mnth\", \"casual\", \"registered\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  hr  holiday  weekday  workingday  weathersit  temp   atemp   hum  \\\n",
       "0       1   0        0        6           0           1  0.24  0.2879  0.81   \n",
       "1       1   1        0        6           0           1  0.22  0.2727  0.80   \n",
       "2       1   2        0        6           0           1  0.22  0.2727  0.80   \n",
       "3       1   3        0        6           0           1  0.24  0.2879  0.75   \n",
       "4       1   4        0        6           0           1  0.24  0.2879  0.75   \n",
       "\n",
       "   windspeed  cnt  \n",
       "0        0.0   16  \n",
       "1        0.0   40  \n",
       "2        0.0   32  \n",
       "3        0.0   13  \n",
       "4        0.0    1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8709, 11)\n",
      "(2177, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.4242</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6205</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.3485</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.1642</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.6212</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.3582</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7225</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.4697</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3305</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.6970</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.2985</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      season  hr  holiday  weekday  workingday  weathersit  temp   atemp  \\\n",
       "4396       4   5        0        6           0           1  0.42  0.4242   \n",
       "6205       1  19        0        2           1           2  0.36  0.3485   \n",
       "2558       2  12        0        2           1           1  0.64  0.6212   \n",
       "7225       2  10        0        4           1           1  0.48  0.4697   \n",
       "3305       3  15        0        0           0           3  0.72  0.6970   \n",
       "\n",
       "       hum  windspeed  cnt  \n",
       "4396  0.71     0.1642    7  \n",
       "6205  0.50     0.1642  281  \n",
       "2558  0.41     0.3582  169  \n",
       "7225  0.63     0.0896  189  \n",
       "3305  0.74     0.2985  326  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df.sample(frac=0.8, random_state=200)  # random state is a seed value\n",
    "test = df.drop(train.index)\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.0000, 5.0000, 0.0000, 6.0000, 0.0000, 1.0000, 0.4200, 0.4242, 0.7100,\n",
       "         0.1642], dtype=torch.float64),\n",
       " tensor(7., dtype=torch.float64))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = data.TensorDataset(\n",
    "    torch.from_numpy(train.values[:, :-1]), torch.from_numpy(train.values[:, -1])\n",
    ")\n",
    "test_dataset = data.TensorDataset(\n",
    "    torch.from_numpy(test.values[:, :-1]), torch.from_numpy(test.values[:, -1])\n",
    ")\n",
    "\n",
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamil/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# GPU operations have a separate seed we also want to set\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Additionally, some operations on a GPU are implemented stochastic for efficiency\n",
    "# We want to ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "            if i < len(layer_sizes) - 2:  # Ostatnia warstwa nie powinna mieć funkcji aktywacji\n",
    "                self.layers.append(nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m model = SimpleClassifier([\u001b[32m10\u001b[39m, \u001b[32m200\u001b[39m, \u001b[32m200\u001b[39m, \u001b[32m1\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/cuda/__init__.py:319\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    318\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mLAZY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[32m    323\u001b[39m _tls.is_initializing = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero."
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "device = torch.device(\"cuda\")\n",
    "model = SimpleClassifier([10, 200, 200, 1])\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSLELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = torch.clamp(y_pred, min=0)  # Ensure non-negative predictions\n",
    "        loss = torch.sqrt(torch.mean((torch.log1p(y_pred) - torch.log1p(y_true)) ** 2))\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_module = RMSLELoss()\n",
    "\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_data_loader = data.DataLoader(\n",
    "    test_dataset, batch_size=len(test_dataset), shuffle=True, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 2.58\n",
      "Epoch: 2, loss: 1.75\n",
      "Epoch: 3, loss: 2.28\n",
      "Epoch: 4, loss: 1.46\n",
      "Epoch: 5, loss: 1.31\n",
      "Epoch: 6, loss: 1.22\n",
      "Epoch: 7, loss: 1.24\n",
      "Epoch: 8, loss: 1.45\n",
      "Epoch: 9, loss: 0.917\n",
      "Epoch: 10, loss: 0.805\n",
      "Epoch: 11, loss: 0.982\n",
      "Epoch: 12, loss: 1.26\n",
      "Epoch: 13, loss: 1.13\n",
      "Epoch: 14, loss: 0.75\n",
      "Epoch: 15, loss: 0.506\n",
      "Epoch: 16, loss: 0.755\n",
      "Epoch: 17, loss: 1.02\n",
      "Epoch: 18, loss: 1.19\n",
      "Epoch: 19, loss: 1.06\n",
      "Epoch: 20, loss: 0.492\n",
      "Epoch: 21, loss: 0.661\n",
      "Epoch: 22, loss: 0.483\n",
      "Epoch: 23, loss: 0.649\n",
      "Epoch: 24, loss: 0.96\n",
      "Epoch: 25, loss: 0.887\n",
      "Epoch: 26, loss: 0.694\n",
      "Epoch: 27, loss: 0.63\n",
      "Epoch: 28, loss: 0.74\n",
      "Epoch: 29, loss: 0.972\n",
      "Epoch: 30, loss: 0.601\n",
      "Epoch: 31, loss: 0.535\n",
      "Epoch: 32, loss: 0.902\n",
      "Epoch: 33, loss: 0.695\n",
      "Epoch: 34, loss: 0.48\n",
      "Epoch: 35, loss: 0.706\n",
      "Epoch: 36, loss: 0.564\n",
      "Epoch: 37, loss: 0.407\n",
      "Epoch: 38, loss: 0.512\n",
      "Epoch: 39, loss: 0.445\n",
      "Epoch: 40, loss: 0.679\n",
      "Epoch: 41, loss: 0.718\n",
      "Epoch: 42, loss: 0.468\n",
      "Epoch: 43, loss: 0.564\n",
      "Epoch: 44, loss: 0.512\n",
      "Epoch: 45, loss: 0.472\n",
      "Epoch: 46, loss: 0.503\n",
      "Epoch: 47, loss: 0.727\n",
      "Epoch: 48, loss: 0.515\n",
      "Epoch: 49, loss: 0.592\n",
      "Epoch: 50, loss: 0.286\n",
      "Epoch: 51, loss: 0.364\n",
      "Epoch: 52, loss: 0.3\n",
      "Epoch: 53, loss: 0.487\n",
      "Epoch: 54, loss: 0.876\n",
      "Epoch: 55, loss: 0.47\n",
      "Epoch: 56, loss: 0.384\n",
      "Epoch: 57, loss: 0.332\n",
      "Epoch: 58, loss: 0.42\n",
      "Epoch: 59, loss: 0.713\n",
      "Epoch: 60, loss: 0.457\n",
      "Epoch: 61, loss: 0.507\n",
      "Epoch: 62, loss: 0.416\n",
      "Epoch: 63, loss: 0.625\n",
      "Epoch: 64, loss: 0.472\n",
      "Epoch: 65, loss: 0.467\n",
      "Epoch: 66, loss: 0.458\n",
      "Epoch: 67, loss: 0.423\n",
      "Epoch: 68, loss: 0.562\n",
      "Epoch: 69, loss: 0.491\n",
      "Epoch: 70, loss: 0.521\n",
      "Epoch: 71, loss: 0.436\n",
      "Epoch: 72, loss: 0.489\n",
      "Epoch: 73, loss: 0.428\n",
      "Epoch: 74, loss: 0.366\n",
      "Epoch: 75, loss: 0.419\n",
      "Epoch: 76, loss: 0.489\n",
      "Epoch: 77, loss: 0.347\n",
      "Epoch: 78, loss: 0.383\n",
      "Epoch: 79, loss: 0.536\n",
      "Epoch: 80, loss: 0.513\n",
      "Epoch: 81, loss: 0.598\n",
      "Epoch: 82, loss: 0.504\n",
      "Epoch: 83, loss: 0.466\n",
      "Epoch: 84, loss: 0.425\n",
      "Epoch: 85, loss: 0.518\n",
      "Epoch: 86, loss: 0.672\n",
      "Epoch: 87, loss: 0.471\n",
      "Epoch: 88, loss: 0.615\n",
      "Epoch: 89, loss: 0.447\n",
      "Epoch: 90, loss: 0.476\n",
      "Epoch: 91, loss: 0.611\n",
      "Epoch: 92, loss: 0.412\n",
      "Epoch: 93, loss: 0.5\n",
      "Epoch: 94, loss: 0.436\n",
      "Epoch: 95, loss: 0.463\n",
      "Epoch: 96, loss: 0.354\n",
      "Epoch: 97, loss: 0.48\n",
      "Epoch: 98, loss: 0.568\n",
      "Epoch: 99, loss: 0.344\n",
      "Epoch: 100, loss: 0.508\n",
      "Epoch: 101, loss: 0.324\n",
      "Epoch: 102, loss: 0.426\n",
      "Epoch: 103, loss: 0.518\n",
      "Epoch: 104, loss: 0.452\n",
      "Epoch: 105, loss: 0.485\n",
      "Epoch: 106, loss: 0.59\n",
      "Epoch: 107, loss: 0.394\n",
      "Epoch: 108, loss: 0.452\n",
      "Epoch: 109, loss: 0.812\n",
      "Epoch: 110, loss: 0.539\n",
      "Epoch: 111, loss: 0.473\n",
      "Epoch: 112, loss: 0.394\n",
      "Epoch: 113, loss: 0.509\n",
      "Epoch: 114, loss: 0.412\n",
      "Epoch: 115, loss: 0.407\n",
      "Epoch: 116, loss: 0.411\n",
      "Epoch: 117, loss: 0.368\n",
      "Epoch: 118, loss: 0.408\n",
      "Epoch: 119, loss: 0.463\n",
      "Epoch: 120, loss: 0.602\n",
      "Epoch: 121, loss: 0.605\n",
      "Epoch: 122, loss: 0.505\n",
      "Epoch: 123, loss: 0.385\n",
      "Epoch: 124, loss: 0.333\n",
      "Epoch: 125, loss: 0.376\n",
      "Epoch: 126, loss: 0.535\n",
      "Epoch: 127, loss: 0.424\n",
      "Epoch: 128, loss: 0.47\n",
      "Epoch: 129, loss: 0.612\n",
      "Epoch: 130, loss: 0.671\n",
      "Epoch: 131, loss: 0.448\n",
      "Epoch: 132, loss: 0.275\n",
      "Epoch: 133, loss: 0.234\n",
      "Epoch: 134, loss: 0.388\n",
      "Epoch: 135, loss: 0.453\n",
      "Epoch: 136, loss: 0.3\n",
      "Epoch: 137, loss: 0.24\n",
      "Epoch: 138, loss: 0.38\n",
      "Epoch: 139, loss: 0.413\n",
      "Epoch: 140, loss: 0.39\n",
      "Epoch: 141, loss: 0.281\n",
      "Epoch: 142, loss: 0.528\n",
      "Epoch: 143, loss: 0.306\n",
      "Epoch: 144, loss: 0.467\n",
      "Epoch: 145, loss: 0.409\n",
      "Epoch: 146, loss: 0.405\n",
      "Epoch: 147, loss: 0.745\n",
      "Epoch: 148, loss: 0.374\n",
      "Epoch: 149, loss: 0.425\n",
      "Epoch: 150, loss: 0.425\n",
      "Epoch: 151, loss: 0.407\n",
      "Epoch: 152, loss: 0.287\n",
      "Epoch: 153, loss: 0.279\n",
      "Epoch: 154, loss: 0.302\n",
      "Epoch: 155, loss: 0.25\n",
      "Epoch: 156, loss: 0.225\n",
      "Epoch: 157, loss: 0.404\n",
      "Epoch: 158, loss: 0.318\n",
      "Epoch: 159, loss: 0.421\n",
      "Epoch: 160, loss: 0.437\n",
      "Epoch: 161, loss: 0.562\n",
      "Epoch: 162, loss: 0.423\n",
      "Epoch: 163, loss: 0.188\n",
      "Epoch: 164, loss: 0.481\n",
      "Epoch: 165, loss: 0.476\n",
      "Epoch: 166, loss: 0.457\n",
      "Epoch: 167, loss: 0.466\n",
      "Epoch: 168, loss: 0.442\n",
      "Epoch: 169, loss: 0.495\n",
      "Epoch: 170, loss: 0.551\n",
      "Epoch: 171, loss: 0.228\n",
      "Epoch: 172, loss: 0.401\n",
      "Epoch: 173, loss: 0.518\n",
      "Epoch: 174, loss: 0.3\n",
      "Epoch: 175, loss: 0.321\n",
      "Epoch: 176, loss: 0.184\n",
      "Epoch: 177, loss: 0.386\n",
      "Epoch: 178, loss: 0.268\n",
      "Epoch: 179, loss: 0.191\n",
      "Epoch: 180, loss: 0.323\n",
      "Epoch: 181, loss: 0.313\n",
      "Epoch: 182, loss: 0.339\n",
      "Epoch: 183, loss: 0.509\n",
      "Epoch: 184, loss: 0.403\n",
      "Epoch: 185, loss: 0.367\n",
      "Epoch: 186, loss: 0.309\n",
      "Epoch: 187, loss: 0.336\n",
      "Epoch: 188, loss: 0.286\n",
      "Epoch: 189, loss: 0.328\n",
      "Epoch: 190, loss: 0.405\n",
      "Epoch: 191, loss: 0.365\n",
      "Epoch: 192, loss: 0.383\n",
      "Epoch: 193, loss: 0.426\n",
      "Epoch: 194, loss: 0.414\n",
      "Epoch: 195, loss: 0.44\n",
      "Epoch: 196, loss: 0.412\n",
      "Epoch: 197, loss: 0.358\n",
      "Epoch: 198, loss: 0.475\n",
      "Epoch: 199, loss: 0.409\n",
      "Epoch: 200, loss: 0.359\n",
      "Epoch: 201, loss: 0.535\n",
      "Epoch: 202, loss: 0.46\n",
      "Epoch: 203, loss: 0.341\n",
      "Epoch: 204, loss: 0.241\n",
      "Epoch: 205, loss: 0.468\n",
      "Epoch: 206, loss: 0.307\n",
      "Epoch: 207, loss: 0.43\n",
      "Epoch: 208, loss: 0.255\n",
      "Epoch: 209, loss: 0.282\n",
      "Epoch: 210, loss: 0.298\n",
      "Epoch: 211, loss: 0.334\n",
      "Epoch: 212, loss: 0.182\n",
      "Epoch: 213, loss: 0.359\n",
      "Epoch: 214, loss: 0.439\n",
      "Epoch: 215, loss: 0.198\n",
      "Epoch: 216, loss: 0.439\n",
      "Epoch: 217, loss: 0.281\n",
      "Epoch: 218, loss: 0.355\n",
      "Epoch: 219, loss: 0.246\n",
      "Epoch: 220, loss: 0.319\n",
      "Epoch: 221, loss: 0.62\n",
      "Epoch: 222, loss: 0.404\n",
      "Epoch: 223, loss: 0.522\n",
      "Epoch: 224, loss: 0.34\n",
      "Epoch: 225, loss: 0.39\n",
      "Epoch: 226, loss: 0.315\n",
      "Epoch: 227, loss: 0.414\n",
      "Epoch: 228, loss: 0.497\n",
      "Epoch: 229, loss: 0.481\n",
      "Epoch: 230, loss: 0.365\n",
      "Epoch: 231, loss: 0.375\n",
      "Epoch: 232, loss: 0.311\n",
      "Epoch: 233, loss: 0.308\n",
      "Epoch: 234, loss: 0.318\n",
      "Epoch: 235, loss: 0.497\n",
      "Epoch: 236, loss: 0.351\n",
      "Epoch: 237, loss: 0.487\n",
      "Epoch: 238, loss: 0.389\n",
      "Epoch: 239, loss: 0.372\n",
      "Epoch: 240, loss: 0.67\n",
      "Epoch: 241, loss: 0.41\n",
      "Epoch: 242, loss: 0.434\n",
      "Epoch: 243, loss: 0.298\n",
      "Epoch: 244, loss: 0.37\n",
      "Epoch: 245, loss: 0.226\n",
      "Epoch: 246, loss: 0.26\n",
      "Epoch: 247, loss: 0.563\n",
      "Epoch: 248, loss: 0.341\n",
      "Epoch: 249, loss: 0.323\n",
      "Epoch: 250, loss: 0.487\n",
      "Epoch: 251, loss: 0.569\n",
      "Epoch: 252, loss: 0.23\n",
      "Epoch: 253, loss: 0.351\n",
      "Epoch: 254, loss: 0.364\n",
      "Epoch: 255, loss: 0.45\n",
      "Epoch: 256, loss: 0.386\n",
      "Epoch: 257, loss: 0.355\n",
      "Epoch: 258, loss: 0.343\n",
      "Epoch: 259, loss: 0.192\n",
      "Epoch: 260, loss: 0.462\n",
      "Epoch: 261, loss: 0.258\n",
      "Epoch: 262, loss: 0.464\n",
      "Epoch: 263, loss: 0.303\n",
      "Epoch: 264, loss: 0.441\n",
      "Epoch: 265, loss: 0.198\n",
      "Epoch: 266, loss: 0.436\n",
      "Epoch: 267, loss: 0.339\n",
      "Epoch: 268, loss: 0.264\n",
      "Epoch: 269, loss: 0.453\n",
      "Epoch: 270, loss: 0.266\n",
      "Epoch: 271, loss: 0.401\n",
      "Epoch: 272, loss: 0.211\n",
      "Epoch: 273, loss: 0.572\n",
      "Epoch: 274, loss: 0.21\n",
      "Epoch: 275, loss: 0.298\n",
      "Epoch: 276, loss: 0.514\n",
      "Epoch: 277, loss: 0.353\n",
      "Epoch: 278, loss: 0.237\n",
      "Epoch: 279, loss: 0.427\n",
      "Epoch: 280, loss: 0.263\n",
      "Epoch: 281, loss: 0.478\n",
      "Epoch: 282, loss: 0.481\n",
      "Epoch: 283, loss: 0.425\n",
      "Epoch: 284, loss: 0.23\n",
      "Epoch: 285, loss: 0.22\n",
      "Epoch: 286, loss: 0.304\n",
      "Epoch: 287, loss: 0.454\n",
      "Epoch: 288, loss: 0.236\n",
      "Epoch: 289, loss: 0.261\n",
      "Epoch: 290, loss: 0.333\n",
      "Epoch: 291, loss: 0.546\n",
      "Epoch: 292, loss: 0.27\n",
      "Epoch: 293, loss: 0.401\n",
      "Epoch: 294, loss: 0.275\n",
      "Epoch: 295, loss: 0.199\n",
      "Epoch: 296, loss: 0.419\n",
      "Epoch: 297, loss: 0.439\n",
      "Epoch: 298, loss: 0.436\n",
      "Epoch: 299, loss: 0.473\n",
      "Epoch: 300, loss: 0.289\n",
      "Epoch: 301, loss: 0.377\n",
      "Epoch: 302, loss: 0.404\n",
      "Epoch: 303, loss: 0.274\n",
      "Epoch: 304, loss: 0.305\n",
      "Epoch: 305, loss: 0.336\n",
      "Epoch: 306, loss: 0.262\n",
      "Epoch: 307, loss: 0.24\n",
      "Epoch: 308, loss: 0.444\n",
      "Epoch: 309, loss: 0.283\n",
      "Epoch: 310, loss: 0.516\n",
      "Epoch: 311, loss: 0.201\n",
      "Epoch: 312, loss: 0.478\n",
      "Epoch: 313, loss: 0.351\n",
      "Epoch: 314, loss: 0.325\n",
      "Epoch: 315, loss: 0.324\n",
      "Epoch: 316, loss: 0.349\n",
      "Epoch: 317, loss: 0.372\n",
      "Epoch: 318, loss: 0.181\n",
      "Epoch: 319, loss: 0.274\n",
      "Epoch: 320, loss: 0.296\n",
      "Epoch: 321, loss: 0.53\n",
      "Epoch: 322, loss: 0.378\n",
      "Epoch: 323, loss: 0.589\n",
      "Epoch: 324, loss: 0.263\n",
      "Epoch: 325, loss: 0.396\n",
      "Epoch: 326, loss: 0.271\n",
      "Epoch: 327, loss: 0.223\n",
      "Epoch: 328, loss: 0.361\n",
      "Epoch: 329, loss: 0.695\n",
      "Epoch: 330, loss: 0.308\n",
      "Epoch: 331, loss: 0.306\n",
      "Epoch: 332, loss: 0.344\n",
      "Epoch: 333, loss: 0.361\n",
      "Epoch: 334, loss: 0.429\n",
      "Epoch: 335, loss: 0.307\n",
      "Epoch: 336, loss: 0.402\n",
      "Epoch: 337, loss: 0.25\n",
      "Epoch: 338, loss: 0.441\n",
      "Epoch: 339, loss: 0.417\n",
      "Epoch: 340, loss: 0.448\n",
      "Epoch: 341, loss: 0.375\n",
      "Epoch: 342, loss: 0.379\n",
      "Epoch: 343, loss: 0.38\n",
      "Epoch: 344, loss: 0.338\n",
      "Epoch: 345, loss: 0.258\n",
      "Epoch: 346, loss: 0.331\n",
      "Epoch: 347, loss: 0.368\n",
      "Epoch: 348, loss: 0.447\n",
      "Epoch: 349, loss: 0.459\n",
      "Epoch: 350, loss: 0.397\n",
      "Epoch: 351, loss: 0.431\n",
      "Epoch: 352, loss: 0.223\n",
      "Epoch: 353, loss: 0.372\n",
      "Epoch: 354, loss: 0.265\n",
      "Epoch: 355, loss: 0.256\n",
      "Epoch: 356, loss: 0.192\n",
      "Epoch: 357, loss: 0.271\n",
      "Epoch: 358, loss: 0.26\n",
      "Epoch: 359, loss: 0.273\n",
      "Epoch: 360, loss: 0.308\n",
      "Epoch: 361, loss: 0.407\n",
      "Epoch: 362, loss: 0.345\n",
      "Epoch: 363, loss: 0.292\n",
      "Epoch: 364, loss: 0.36\n",
      "Epoch: 365, loss: 0.281\n",
      "Epoch: 366, loss: 0.273\n",
      "Epoch: 367, loss: 0.446\n",
      "Epoch: 368, loss: 0.409\n",
      "Epoch: 369, loss: 0.3\n",
      "Epoch: 370, loss: 0.388\n",
      "Epoch: 371, loss: 0.239\n",
      "Epoch: 372, loss: 0.317\n",
      "Epoch: 373, loss: 0.31\n",
      "Epoch: 374, loss: 0.292\n",
      "Epoch: 375, loss: 0.256\n",
      "Epoch: 376, loss: 0.372\n",
      "Epoch: 377, loss: 0.403\n",
      "Epoch: 378, loss: 0.468\n",
      "Epoch: 379, loss: 0.388\n",
      "Epoch: 380, loss: 0.303\n",
      "Epoch: 381, loss: 0.234\n",
      "Epoch: 382, loss: 0.225\n",
      "Epoch: 383, loss: 0.252\n",
      "Epoch: 384, loss: 0.235\n",
      "Epoch: 385, loss: 0.283\n",
      "Epoch: 386, loss: 0.219\n",
      "Epoch: 387, loss: 0.354\n",
      "Epoch: 388, loss: 0.249\n",
      "Epoch: 389, loss: 0.218\n",
      "Epoch: 390, loss: 0.329\n",
      "Epoch: 391, loss: 0.366\n",
      "Epoch: 392, loss: 0.416\n",
      "Epoch: 393, loss: 0.309\n",
      "Epoch: 394, loss: 0.241\n",
      "Epoch: 395, loss: 0.261\n",
      "Epoch: 396, loss: 0.299\n",
      "Epoch: 397, loss: 0.446\n",
      "Epoch: 398, loss: 0.4\n",
      "Epoch: 399, loss: 0.29\n",
      "Epoch: 400, loss: 0.252\n",
      "Epoch: 401, loss: 0.315\n",
      "Epoch: 402, loss: 0.333\n",
      "Epoch: 403, loss: 0.333\n",
      "Epoch: 404, loss: 0.296\n",
      "Epoch: 405, loss: 0.43\n",
      "Epoch: 406, loss: 0.236\n",
      "Epoch: 407, loss: 0.403\n",
      "Epoch: 408, loss: 0.374\n",
      "Epoch: 409, loss: 0.362\n",
      "Epoch: 410, loss: 0.217\n",
      "Epoch: 411, loss: 0.3\n",
      "Epoch: 412, loss: 0.462\n",
      "Epoch: 413, loss: 0.412\n",
      "Epoch: 414, loss: 0.277\n",
      "Epoch: 415, loss: 0.346\n",
      "Epoch: 416, loss: 0.444\n",
      "Epoch: 417, loss: 0.325\n",
      "Epoch: 418, loss: 0.391\n",
      "Epoch: 419, loss: 0.288\n",
      "Epoch: 420, loss: 0.22\n",
      "Epoch: 421, loss: 0.401\n",
      "Epoch: 422, loss: 0.293\n",
      "Epoch: 423, loss: 0.373\n",
      "Epoch: 424, loss: 0.388\n",
      "Epoch: 425, loss: 0.263\n",
      "Epoch: 426, loss: 0.39\n",
      "Epoch: 427, loss: 0.493\n",
      "Epoch: 428, loss: 0.405\n",
      "Epoch: 429, loss: 0.257\n",
      "Epoch: 430, loss: 0.573\n",
      "Epoch: 431, loss: 0.332\n",
      "Epoch: 432, loss: 0.705\n",
      "Epoch: 433, loss: 0.354\n",
      "Epoch: 434, loss: 0.365\n",
      "Epoch: 435, loss: 0.293\n",
      "Epoch: 436, loss: 0.286\n",
      "Epoch: 437, loss: 0.259\n",
      "Epoch: 438, loss: 0.27\n",
      "Epoch: 439, loss: 0.465\n",
      "Epoch: 440, loss: 0.395\n",
      "Epoch: 441, loss: 0.508\n",
      "Epoch: 442, loss: 0.318\n",
      "Epoch: 443, loss: 0.285\n",
      "Epoch: 444, loss: 0.361\n",
      "Epoch: 445, loss: 0.32\n",
      "Epoch: 446, loss: 0.297\n",
      "Epoch: 447, loss: 0.502\n",
      "Epoch: 448, loss: 0.314\n",
      "Epoch: 449, loss: 0.263\n",
      "Epoch: 450, loss: 0.272\n",
      "Epoch: 451, loss: 0.324\n",
      "Epoch: 452, loss: 0.299\n",
      "Epoch: 453, loss: 0.357\n",
      "Epoch: 454, loss: 0.253\n",
      "Epoch: 455, loss: 0.208\n",
      "Epoch: 456, loss: 0.406\n",
      "Epoch: 457, loss: 0.251\n",
      "Epoch: 458, loss: 0.278\n",
      "Epoch: 459, loss: 0.279\n",
      "Epoch: 460, loss: 0.229\n",
      "Epoch: 461, loss: 0.232\n",
      "Epoch: 462, loss: 0.272\n",
      "Epoch: 463, loss: 0.191\n",
      "Epoch: 464, loss: 0.268\n",
      "Epoch: 465, loss: 0.344\n",
      "Epoch: 466, loss: 0.326\n",
      "Epoch: 467, loss: 0.332\n",
      "Epoch: 468, loss: 0.492\n",
      "Epoch: 469, loss: 0.39\n",
      "Epoch: 470, loss: 0.235\n",
      "Epoch: 471, loss: 0.418\n",
      "Epoch: 472, loss: 0.356\n",
      "Epoch: 473, loss: 0.381\n",
      "Epoch: 474, loss: 0.281\n",
      "Epoch: 475, loss: 0.282\n",
      "Epoch: 476, loss: 0.239\n",
      "Epoch: 477, loss: 0.332\n",
      "Epoch: 478, loss: 0.475\n",
      "Epoch: 479, loss: 0.217\n",
      "Epoch: 480, loss: 0.301\n",
      "Epoch: 481, loss: 0.376\n",
      "Epoch: 482, loss: 0.314\n",
      "Epoch: 483, loss: 0.382\n",
      "Epoch: 484, loss: 0.401\n",
      "Epoch: 485, loss: 0.34\n",
      "Epoch: 486, loss: 0.277\n",
      "Epoch: 487, loss: 0.269\n",
      "Epoch: 488, loss: 0.385\n",
      "Epoch: 489, loss: 0.337\n",
      "Epoch: 490, loss: 0.457\n",
      "Epoch: 491, loss: 0.294\n",
      "Epoch: 492, loss: 0.161\n",
      "Epoch: 493, loss: 0.314\n",
      "Epoch: 494, loss: 0.237\n",
      "Epoch: 495, loss: 0.307\n",
      "Epoch: 496, loss: 0.317\n",
      "Epoch: 497, loss: 0.321\n",
      "Epoch: 498, loss: 0.276\n",
      "Epoch: 499, loss: 0.367\n",
      "Epoch: 500, loss: 0.474\n",
      "Epoch: 501, loss: 0.325\n",
      "Epoch: 502, loss: 0.151\n",
      "Epoch: 503, loss: 0.339\n",
      "Epoch: 504, loss: 0.342\n",
      "Epoch: 505, loss: 0.301\n",
      "Epoch: 506, loss: 0.348\n",
      "Epoch: 507, loss: 0.487\n",
      "Epoch: 508, loss: 0.371\n",
      "Epoch: 509, loss: 0.376\n",
      "Epoch: 510, loss: 0.293\n",
      "Epoch: 511, loss: 0.377\n",
      "Epoch: 512, loss: 0.272\n",
      "Epoch: 513, loss: 0.292\n",
      "Epoch: 514, loss: 0.351\n",
      "Epoch: 515, loss: 0.342\n",
      "Epoch: 516, loss: 0.326\n",
      "Epoch: 517, loss: 0.315\n",
      "Epoch: 518, loss: 0.216\n",
      "Epoch: 519, loss: 0.342\n",
      "Epoch: 520, loss: 0.606\n",
      "Epoch: 521, loss: 0.395\n",
      "Epoch: 522, loss: 0.246\n",
      "Epoch: 523, loss: 0.296\n",
      "Epoch: 524, loss: 0.268\n",
      "Epoch: 525, loss: 0.267\n",
      "Epoch: 526, loss: 0.408\n",
      "Epoch: 527, loss: 0.221\n",
      "Epoch: 528, loss: 0.345\n",
      "Epoch: 529, loss: 0.417\n",
      "Epoch: 530, loss: 0.265\n",
      "Epoch: 531, loss: 0.249\n",
      "Epoch: 532, loss: 0.388\n",
      "Epoch: 533, loss: 0.284\n",
      "Epoch: 534, loss: 0.282\n",
      "Epoch: 535, loss: 0.296\n",
      "Epoch: 536, loss: 0.334\n",
      "Epoch: 537, loss: 0.348\n",
      "Epoch: 538, loss: 0.394\n",
      "Epoch: 539, loss: 0.354\n",
      "Epoch: 540, loss: 0.283\n",
      "Epoch: 541, loss: 0.32\n",
      "Epoch: 542, loss: 0.46\n",
      "Epoch: 543, loss: 0.23\n",
      "Epoch: 544, loss: 0.352\n",
      "Epoch: 545, loss: 0.327\n",
      "Epoch: 546, loss: 0.219\n",
      "Epoch: 547, loss: 0.266\n",
      "Epoch: 548, loss: 0.219\n",
      "Epoch: 549, loss: 0.314\n",
      "Epoch: 550, loss: 0.207\n",
      "Epoch: 551, loss: 0.532\n",
      "Epoch: 552, loss: 0.295\n",
      "Epoch: 553, loss: 0.385\n",
      "Epoch: 554, loss: 0.289\n",
      "Epoch: 555, loss: 0.217\n",
      "Epoch: 556, loss: 0.279\n",
      "Epoch: 557, loss: 0.255\n",
      "Epoch: 558, loss: 0.293\n",
      "Epoch: 559, loss: 0.237\n",
      "Epoch: 560, loss: 0.311\n",
      "Epoch: 561, loss: 0.245\n",
      "Epoch: 562, loss: 0.315\n",
      "Epoch: 563, loss: 0.297\n",
      "Epoch: 564, loss: 0.246\n",
      "Epoch: 565, loss: 0.22\n",
      "Epoch: 566, loss: 0.257\n",
      "Epoch: 567, loss: 0.287\n",
      "Epoch: 568, loss: 0.254\n",
      "Epoch: 569, loss: 0.29\n",
      "Epoch: 570, loss: 0.405\n",
      "Epoch: 571, loss: 0.222\n",
      "Epoch: 572, loss: 0.292\n",
      "Epoch: 573, loss: 0.307\n",
      "Epoch: 574, loss: 0.256\n",
      "Epoch: 575, loss: 0.302\n",
      "Epoch: 576, loss: 0.428\n",
      "Epoch: 577, loss: 0.246\n",
      "Epoch: 578, loss: 0.42\n",
      "Epoch: 579, loss: 0.331\n",
      "Epoch: 580, loss: 0.213\n",
      "Epoch: 581, loss: 0.319\n",
      "Epoch: 582, loss: 0.35\n",
      "Epoch: 583, loss: 0.226\n",
      "Epoch: 584, loss: 0.364\n",
      "Epoch: 585, loss: 0.413\n",
      "Epoch: 586, loss: 0.3\n",
      "Epoch: 587, loss: 0.244\n",
      "Epoch: 588, loss: 0.26\n",
      "Epoch: 589, loss: 0.385\n",
      "Epoch: 590, loss: 0.393\n",
      "Epoch: 591, loss: 0.314\n",
      "Epoch: 592, loss: 0.322\n",
      "Epoch: 593, loss: 0.287\n",
      "Epoch: 594, loss: 0.29\n",
      "Epoch: 595, loss: 0.432\n",
      "Epoch: 596, loss: 0.42\n",
      "Epoch: 597, loss: 0.46\n",
      "Epoch: 598, loss: 0.402\n",
      "Epoch: 599, loss: 0.39\n",
      "Epoch: 600, loss: 0.167\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(600):\n",
    "    for data_inputs, data_labels in train_data_loader:\n",
    "        ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
    "        data_inputs = data_inputs.to(device)\n",
    "        data_labels = data_labels.to(device)\n",
    "\n",
    "        ## Step 2: Run the model on the input data\n",
    "        preds = model(data_inputs.float())\n",
    "        preds = preds.squeeze(\n",
    "            dim=1\n",
    "        )  # Output is [Batch size, 1], but we want [Batch size]\n",
    "\n",
    "        ## Step 3: Calculate the loss\n",
    "        loss = loss_module(preds, data_labels.float())\n",
    "\n",
    "        ## Step 4: Perform backpropagation\n",
    "        # Before calculating the gradients, we need to ensure that they are all zero.\n",
    "        # The gradients would not be overwritten, but actually added to the existing ones.\n",
    "        optimizer.zero_grad()\n",
    "        # Perform backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        ## Step 5: Update the parameters\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, loss: {loss.item():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y_true,y_pred):\n",
    "    n = len(y_true)\n",
    "    msle = np.mean([(np.log(max(y_pred[i],0) + 1) - np.log(y_true[i] + 1)) ** 2.0 for i in range(n)])\n",
    "    return np.sqrt(msle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSLE Error: 0.4105\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set model to eval mode\n",
    "rmsle_errors = []\n",
    "\n",
    "with torch.no_grad():  # Deactivate gradients for the following code\n",
    "    for data_inputs, data_labels in test_data_loader:\n",
    "        # Move data to the correct device\n",
    "        data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
    "\n",
    "        # Model predictions\n",
    "        preds = model(data_inputs.float()).squeeze(dim=1)\n",
    "\n",
    "        # Convert predictions and labels to NumPy for RMSLE calculation\n",
    "        preds_np = preds.cpu().numpy()\n",
    "        labels_np = data_labels.cpu().numpy()\n",
    "\n",
    "        # Compute RMSLE for this batch\n",
    "        msle = rmsle(labels_np, preds_np)\n",
    "        rmsle_errors.append(msle)\n",
    "\n",
    "# Compute the mean RMSLE across all batches\n",
    "mean_rmsle = np.mean(rmsle_errors)\n",
    "print(f\"Mean RMSLE Error: {mean_rmsle:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.3881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.1642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.1642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dteday  season  yr  mnth  hr  holiday  weekday  workingday  weathersit  \\\n",
       "0  2011-01-20       1   0     1   0        0        4           1           1   \n",
       "1  2011-01-20       1   0     1   1        0        4           1           1   \n",
       "2  2011-01-20       1   0     1   2        0        4           1           1   \n",
       "3  2011-01-20       1   0     1   3        0        4           1           1   \n",
       "4  2011-01-20       1   0     1   4        0        4           1           1   \n",
       "\n",
       "   temp   atemp   hum  windspeed  \n",
       "0  0.26  0.2273  0.56     0.3881  \n",
       "1  0.26  0.2727  0.56     0.0000  \n",
       "2  0.26  0.2727  0.56     0.0000  \n",
       "3  0.26  0.2576  0.56     0.1642  \n",
       "4  0.26  0.2576  0.56     0.1642  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval = pd.read_csv(\n",
    "    \"evaluation_data.csv\",\n",
    "    delimiter=\",\",\n",
    ")\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17.4314,  8.6240,  5.3688,  ..., 70.5292, 48.4567, 31.7086],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "df_eval = df_eval.drop(columns=[\"dteday\", \"yr\", \"mnth\"])\n",
    "\n",
    "# Data without the target column\n",
    "data_inputs = torch.from_numpy(df_eval.values)\n",
    "data_inputs = data_inputs.to(device)\n",
    "\n",
    "# Model predictions\n",
    "with torch.no_grad():\n",
    "    preds = model(data_inputs.float()).squeeze(dim=1)\n",
    "\n",
    "print(preds)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "pd.DataFrame(preds.cpu()).to_csv(\"sroda_cisek_wiecyk_preds.csv\", index=False, header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
