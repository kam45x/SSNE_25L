{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2feb905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/tscratch/people/plgmilosz03/conda-envs/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/net/tscratch/people/plgmilosz03/conda-envs/env/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/net/tscratch/people/plgmilosz03/conda-envs/env/lib/python3.12/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/net/tscratch/people/plgmilosz03/conda-envs/env/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/net/tscratch/people/plgmilosz03/conda-envs/env/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "from peft import LoraConfig, TaskType, PeftModel, get_peft_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    CLIPModel,\n",
    "    CLIPProcessor,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560354e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1592c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd3292aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=2, output_attentions=False, output_hidden_states=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60edd25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = emoji.replace_emoji(text, replace='')\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'^RT\\s+', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "        text = re.sub(r'www\\.\\S+', '', text)\n",
    "        text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "        text = text.strip()\n",
    "                \n",
    "        return text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.preprocess_text(str(self.sentences[idx]))\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,           \n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f408dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dla mnie faworytem do tytuu bdzie Cracovia. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@anonymized_account @anonymized_account Brawo ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@anonymized_account @anonymized_account Super,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@anonymized_account @anonymized_account Musi. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Odrzut natychmiastowy, kwana mina, mam problem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  Dla mnie faworytem do tytuu bdzie Cracovia. ...      0\n",
       "1  @anonymized_account @anonymized_account Brawo ...      0\n",
       "2  @anonymized_account @anonymized_account Super,...      0\n",
       "3  @anonymized_account @anonymized_account Musi. ...      0\n",
       "4    Odrzut natychmiastowy, kwana mina, mam problem      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"hate_train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e5daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['sentence'].tolist()\n",
    "labels = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f555506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozkad klas przed oversampling:\n",
      "label\n",
      "0    9190\n",
      "1     851\n",
      "Name: count, dtype: int64\n",
      "Procent klasy 1: 8.48%\n",
      "Cakowita liczba pr贸bek: 10041\n",
      "\n",
      "Rozkad klas po oversampling:\n",
      "label\n",
      "0    9190\n",
      "1    4255\n",
      "Name: count, dtype: int64\n",
      "Procent klasy 1: 31.65%\n",
      "Cakowita liczba pr贸bek: 13445\n",
      "\n",
      "Po train_test_split:\n",
      "Train: 10756 pr贸bek\n",
      "Val: 2689 pr贸bek\n"
     ]
    }
   ],
   "source": [
    "print(\"Rozkad klas przed oversampling:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"Procent klasy 1: {(df['label']==1).sum() / len(df) * 100:.2f}%\")\n",
    "print(f\"Cakowita liczba pr贸bek: {len(df)}\")\n",
    "\n",
    "multiplier = 5\n",
    "\n",
    "positive_samples = df[df['label'] == 1]\n",
    "\n",
    "duplicated_positives = pd.concat([positive_samples] * (multiplier - 1), ignore_index=True)\n",
    "\n",
    "df_oversampled = pd.concat([df, duplicated_positives], ignore_index=True)\n",
    "\n",
    "df_oversampled = df_oversampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nRozkad klas po oversampling:\")\n",
    "print(df_oversampled['label'].value_counts())\n",
    "print(f\"Procent klasy 1: {(df_oversampled['label']==1).sum() / len(df_oversampled) * 100:.2f}%\")\n",
    "print(f\"Cakowita liczba pr贸bek: {len(df_oversampled)}\")\n",
    "\n",
    "sentences = df_oversampled['sentence'].tolist()\n",
    "labels = df_oversampled['label'].tolist()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    sentences, \n",
    "    labels, \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"\\nPo train_test_split:\")\n",
    "print(f\"Train: {len(X_train)} pr贸bek\")\n",
    "print(f\"Val: {len(X_val)} pr贸bek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb933130",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HateSpeechDataset(sentences=X_train, labels=y_train, tokenizer=tokenizer)\n",
    "eval_dataset = HateSpeechDataset(sentences=X_val, labels=y_val, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "009df412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 184,423,682\n",
      "DebertaV2ForSequenceClassification(\n",
      "  (deberta): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (pooler): ContextPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): StableDropout()\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): StableDropout()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c879c061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DebertaV2ForSequenceClassification(\n",
      "      (deberta): DebertaV2Model(\n",
      "        (embeddings): DebertaV2Embeddings(\n",
      "          (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "          (dropout): StableDropout()\n",
      "        )\n",
      "        (encoder): DebertaV2Encoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x DebertaV2Layer(\n",
      "              (attention): DebertaV2Attention(\n",
      "                (self): DisentangledSelfAttention(\n",
      "                  (query_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (pos_dropout): StableDropout()\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "                (output): DebertaV2SelfOutput(\n",
      "                  (dense): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "              )\n",
      "              (intermediate): DebertaV2Intermediate(\n",
      "                (dense): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=3072, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DebertaV2Output(\n",
      "                (dense): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3072, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (rel_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (pooler): ContextPooler(\n",
      "        (dense): lora.Linear(\n",
      "          (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (dropout): StableDropout()\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"query_proj\", \n",
    "        \"key_proj\", \n",
    "        \"value_proj\", \n",
    "        \"dense\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_results\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=25,\n",
    "    per_device_eval_batch_size=10,\n",
    "    logging_dir=\"./lora_logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-4,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f7d823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 2,680,322\n"
     ]
    }
   ],
   "source": [
    "# Print number of trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8194453b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8620' max='8620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8620/8620 56:56, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.577400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.482400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.415300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.345700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.304700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.255100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.159900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.102300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.107200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.081900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.070200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./lora_results/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-2000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-2500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-3000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-3500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-4000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-4500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-5000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-5500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-6000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8620, training_loss=0.06875721449929174, metrics={'train_runtime': 3417.5301, 'train_samples_per_second': 62.946, 'train_steps_per_second': 2.522, 'total_flos': 5.837275244101632e+16, 'train_loss': 0.06875721449929174, 'epoch': 20.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c02e2fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./lora_adapters/tokenizer_config.json',\n",
       " './lora_adapters/special_tokens_map.json',\n",
       " './lora_adapters/spm.model',\n",
       " './lora_adapters/added_tokens.json',\n",
       " './lora_adapters/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./lora_adapters\")\n",
    "tokenizer.save_pretrained(\"./lora_adapters\")\n",
    "\n",
    "# base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"microsoft/deberta-v3-base\",\n",
    "#     num_labels=2,\n",
    "#     output_attentions=False,\n",
    "#     output_hidden_states=False,\n",
    "# )\n",
    "\n",
    "# model = PeftModel.from_pretrained(base_model, \"./lora_adapters\")\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Zaaduj tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./lora_adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76853560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.9803\n",
      "Precision:       0.9414\n",
      "Recall:          1.0000\n",
      "F1-Score:        0.9698\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Evaluate without gradient computation\n",
    "with torch.no_grad():\n",
    "    for batch in eval_dataloader:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        # Store predictions and labels\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Model Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision:       {precision:.4f}\")\n",
    "print(f\"Recall:          {recall:.4f}\")\n",
    "print(f\"F1-Score:        {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a02c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, max_length=256):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        text = emoji.replace_emoji(text, replace='')\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'^RT\\s+', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "        text = re.sub(r'www\\.\\S+', '', text)\n",
    "        text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "        text = text.strip()\n",
    "                \n",
    "        return text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.preprocess_text(str(self.sentences[idx]))\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,           \n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd5f010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Przykadowe predykcje:\n",
      "1. '@anonymized_account Spoko, jak im Duda z Morawieckim zam贸wi po pi piw to wszystko bdzie ok....' -> 0\n",
      "2. '@anonymized_account @anonymized_account Ale on tu nie mia szans jej zagrania, a ta 'proba' to czysta prowizorka....' -> 0\n",
      "3. '@anonymized_account No czy Prezes nie mia racji, m贸wic,ze to s zdradzieckie mordy? No czy nie mia racji?...' -> 0\n",
      "4. '@anonymized_account @anonymized_account Przecie偶 to nawet nie jest przewrotka ...' -> 0\n",
      "5. '@anonymized_account @anonymized_account Owszem podatki tak. Ale nie w takich okolicznociach. Czemu Maysza odpalili z teamu Orlen?...' -> 0\n",
      "6. '@anonymized_account @anonymized_account skd wiesz jaki Skendija ma bud偶et skoro m贸wisz 偶e jest bogatsza ? Tylko dw贸ch zawodnik贸w pono dobrze zarabia....' -> 0\n",
      "7. 'Z tego, co widz, to kibice Widzewa maj szczcie, 偶e trwa mundial. Dziki temu og贸lnopolska szydera jest tylko z Argentyny i Messiego....' -> 0\n",
      "8. '@anonymized_account @anonymized_account @anonymized_account Na utrzymanie wasnej armii 2% PKB, tyle 偶e teraz to jedna wielka ciema...' -> 0\n",
      "9. 'Przypomnijcie mi ze musz jeszcze suszark spakowa...' -> 0\n",
      "10. 'Czy Adam ju偶 nie 偶yje? Jeli tak, to jak rozwizali jego wtek?  #nadobreinaze...' -> 0\n",
      "11. '@anonymized_account Nie zmieci si w kadrze to dosta wolne, bo przecie偶 teraz bdzie przerwa na kadr....' -> 0\n",
      "12. 'Jajka na mikko czy na twardo? Jeli jecie jajka ofc...' -> 0\n",
      "13. '@anonymized_account @anonymized_account Ty wiesz lepiej. Ja wiem, 偶e nawet wiceprezydentem nie bdziesz na 100%...' -> 0\n",
      "14. '@anonymized_account @anonymized_account Kto mieczem wojuje, ten od pochwy ginie...' -> 0\n",
      "15. '@anonymized_account Mo偶e si myl, ale tego Szwajcara po lewej to w og贸le m贸g dobrze nie widzie :)...' -> 0\n",
      "16. '@anonymized_account Zaraz Wisa pewnie dopaci Legii, 偶eby tylko ogosili, 偶e kupili go za 2mln euro ...' -> 0\n",
      "17. '@anonymized_account Dopiero teraz jest sterylne dziki maseczce....' -> 0\n",
      "18. '@anonymized_account Nie mam informacji na ten temat....' -> 0\n",
      "19. '@anonymized_account @anonymized_account @anonymized_account Absolutnie nie o te kryteria chodzi....' -> 0\n",
      "20. '@anonymized_account R贸wnie praktyczne, jak jazda na rowerze g贸rskim w za du偶ych o rozmiar szpilkach dwunastkach....' -> 0\n",
      "21. 'Nowa dziewczyna Liama ma serio 18 lat?...' -> 0\n",
      "22. '@anonymized_account @anonymized_account @anonymized_account W takim razie sie zapedzilem, i tak nie daje ban贸w, nawet botom...' -> 0\n",
      "23. '@anonymized_account wieczna zdrada, nie zdrada, trzeba rozmawia, pierdolenie od rzeczy...' -> 0\n",
      "24. '@anonymized_account na Westerplatte 13 jest sklep Lego...' -> 0\n",
      "25. '@anonymized_account Tej szmaty si nie komentuje...' -> 1\n",
      "26. '@anonymized_account A ty taki dziennikarz, 偶e nie wiesz co oznacza znak zapytania ...' -> 0\n",
      "27. '@anonymized_account @anonymized_account I dziki temu mamy dzi cesarzow... Gorco pozdrawiam!...' -> 0\n",
      "28. 'Jutro media wsp贸czesne, wf i retoryka, sounds gr8...' -> 0\n",
      "29. '@anonymized_account potem nie wytykaj innym 偶e Polacy to mistrzowie w narzekaniu...' -> 0\n",
      "30. '@anonymized_account Mogem, mogem, mogem co zrobi zamiast cigle wali konia...' -> 0\n",
      "31. '@anonymized_account @anonymized_account To Pani Krysia jest Twoj Ojczyzn?...' -> 0\n",
      "32. '@anonymized_account Dokadnie, pisdzielstwo nie ma prawa rozpierdala systemu,  sdownictwa nie majc wikszoci...' -> 1\n",
      "33. '@anonymized_account Nie wiedziaam, 偶e jest takie pastwo jak HINY....' -> 0\n",
      "34. 'Robert wszed do azienki a to znaczy ze mog sobie pomarzy o prysznicu przynajmniej przez jakie 1,5 godziny jeszcze ugh...' -> 0\n",
      "35. 'Mylaem, 偶e ten mecz bdzie dla Brazylijczyk贸w trudniejszy....' -> 0\n",
      "36. '@anonymized_account Zgadzam si, aczkolwiek nie ma takiej mocy ofensywnej, wic mo偶e by to mecz na 1:0....' -> 0\n",
      "37. '@anonymized_account Mi chodzi o to, 偶e wg artyku贸w sprawa te偶 jest do rozwizania kontraktu. Powody oczywicie s inne i widz r贸偶nice....' -> 0\n",
      "38. '@anonymized_account Adrian Juda, figurant WSI i  lobby 偶ydowskiego...' -> 1\n",
      "39. '@anonymized_account Pewnie s na 180 km...' -> 0\n",
      "40. '@anonymized_account Nieprawda. Normalnie tak si nie \\\"tnie\\\"....' -> 0\n",
      "41. '@anonymized_account Ju偶 mamy si mia ?...' -> 0\n",
      "42. '@anonymized_account Chop z najwikszymi kompleksami pisze co o kompleksach u innych ...' -> 0\n",
      "43. '@anonymized_account Michniewicza czy Matusiaka te偶 lubisz, ale to 偶e handlowali meczami to ju偶 mo偶esz znie ...' -> 0\n",
      "44. '@anonymized_account @anonymized_account @anonymized_account w nagrode wypij m贸j syrop! :D...' -> 0\n",
      "45. 'Co si dzieje atak bot贸w z ka偶dej strony...' -> 0\n",
      "46. 'A na 偶ywo brzmi jak niebo...' -> 0\n",
      "47. '@anonymized_account Bez problemu Czechy i Litwa nas wyprzedziy mimo tego historycznego wzrostu...' -> 0\n",
      "48. '@anonymized_account warunkowo na pewno by dosta pozwolenie.To przecie偶 III liga...' -> 0\n",
      "49. '@anonymized_account @anonymized_account @anonymized_account @anonymized_account Chtnie zrobie Wam syrop hawajski z ananasem :D p贸jd do szefa z t propozycj ...' -> 0\n",
      "50. '@anonymized_account @anonymized_account po fryzjerze zawsze si dobrze wyglda. Chwil....' -> 0\n",
      "51. '@anonymized_account Za takie akcje to tylko legia ma wyczno u polskich sdzi贸w na karne tak偶e Jaga Luz ...' -> 0\n",
      "52. '@anonymized_account Wida ludziom w Sczu tak pasuje. Ja bym mu w gb naplu...' -> 1\n",
      "53. '@anonymized_account 呕e si ten operator nie wyo偶y......' -> 0\n",
      "54. 'na b贸l garda najlepsza w贸deczka...' -> 0\n",
      "55. '@anonymized_account Od sdzi贸w z postpowaniami dyscyplinarnymi kandydujcymi do KRS...' -> 0\n",
      "56. '@anonymized_account @anonymized_account mamy bra kogo bo inni bior ?...' -> 0\n",
      "57. '@anonymized_account Droga p.Kamilko! Leczy si . Leczy p贸ki czas...' -> 0\n",
      "58. '@anonymized_account powinna odpowiedzie za dziaanie na szkod Polski i obywateli...' -> 0\n",
      "59. 'RT @anonymized_account @anonymized_account powinna odpowiedzie za dziaanie na szkod Polski i obywateli...' -> 0\n",
      "60. '@anonymized_account Awww dzikuje z caego serduszka...' -> 0\n",
      "61. '@anonymized_account Dubaj wzorem, czyli jednak islam, salam alejkum Ciemkiewicz...' -> 1\n",
      "62. '@anonymized_account @anonymized_account @anonymized_account Najbardziej to on jest wolny od m贸zgu....' -> 1\n",
      "63. 'musz podjecha dzisiaj do mechanika...' -> 0\n",
      "64. '@anonymized_account 2/2 a p贸藕niej si zastanawiaj po co Wile akademia i wychwalaj akademie Legii i ich stawianie na modzie偶...' -> 0\n",
      "65. '@anonymized_account   P贸g贸wek Wieliski, wymyli sobie p贸autorytaryzm!...' -> 0\n",
      "66. 'RT @anonymized_account @anonymized_account   P贸g贸wek Wieliski, wymyli sobie p贸autorytaryzm!...' -> 0\n",
      "67. '@anonymized_account Ale najpierw Sadurskiego do Australii...' -> 0\n",
      "68. '@anonymized_account A w koci贸ku ju偶 bye?...' -> 0\n",
      "69. '@anonymized_account A wy macie? Razem z Piotrowicz en i reszt?...' -> 0\n",
      "70. 'Dzisiaj musiaam pracowa w grupie z chopakami z roku i mylaam ze wyjd z siebie i z tej sali I mean jak mo偶na by tak infantylnym...' -> 0\n",
      "71. 'Co za bekot ten halicki.  #Woronicza17...' -> 0\n",
      "72. 'niech mnie kto zabierze w g贸ry...' -> 0\n",
      "73. 'Decyzj spikera brytyjskiej Izby Gmin noszenie krawat贸w przez pos贸w i reporter贸w podczas posiedze parlamentu nie bdzie ju偶 obowizkowe ...' -> 0\n",
      "74. '@anonymized_account @anonymized_account Podobnie, jak m贸j w odniesieniu do mskiej kreatywnoci ...' -> 0\n",
      "75. '@anonymized_account @anonymized_account @anonymized_account @anonymized_account Ja nie wiem, nie dowiadywaem si :)...' -> 0\n",
      "76. '@anonymized_account Ciebie te偶 nie ma, wikarku...' -> 0\n",
      "77. '@anonymized_account @anonymized_account Sprawdzam na flightradarze, samolot \\\"ju偶\\\" wlecia do Polski....' -> 0\n",
      "78. 'Z kolei Luka Gugeszaszwili w piknym stylu wyj karnego w meczu Finlandia - Gruzja (1:2)....' -> 0\n",
      "79. '@anonymized_account @anonymized_account jak na nokia 3310 to spoko :)...' -> 0\n",
      "80. '@anonymized_account Chyba, 偶e tradycyjnie wylosujemy Zakaukazie i mecze bd o 16 naszego czasu ...' -> 0\n",
      "81. 'M偶czy藕ni siedzcy przed gabinetem u laryngologa nie czekaj na wizyt; oni walcz o przetrwanie gatunku....' -> 0\n",
      "82. '@anonymized_account a ja zao偶 fitbloga ...' -> 0\n",
      "83. '@anonymized_account czyli dla Legii te偶 nie ma miejsca @anonymized_account...' -> 0\n",
      "84. '@anonymized_account @anonymized_account @anonymized_account Podstawowe zadanie ka偶dego ksidza to narzucanie wiary....' -> 0\n",
      "85. '@anonymized_account Mam podobny i m贸wili ze to ciemny blond chocia偶 rozjaniam i teraz faktycznie jestem blondynka...' -> 0\n",
      "86. '@anonymized_account @anonymized_account Na szczcie @anonymized_account i  jego partia znikn  wkr贸tce \\nze sceny politycznej. Brawo!...' -> 0\n",
      "87. '@anonymized_account Otworzysz nastpny? \\nPrzyjdzie na ciebie pora popapracu. TS czeka....' -> 0\n",
      "88. '@anonymized_account @anonymized_account @anonymized_account Spodobay mi piosenki kt贸re piewaj...' -> 0\n",
      "89. 'Fav albo rt to zrobi Wam indy!!! Nie wiem czy wszystkim, zale偶y kogo bd kojarzy ヰヰ...' -> 0\n",
      "90. '@anonymized_account @anonymized_account @anonymized_account I wojna Gadowskiego z psychiatr....' -> 0\n",
      "91. 'youngblood to piosenka moich wakacji...' -> 0\n",
      "92. '@anonymized_account ostatnio pewnie chciae usysze od Pazdana jacy to kibice Wisy 藕li a tu dupa...' -> 0\n",
      "93. '@anonymized_account Dziecko teraz walczy o 偶ycie bo przecie偶 mam takie straszne obra偶enia...' -> 0\n",
      "94. '@anonymized_account Dokadnie, a czytam w niekt贸rych miejscach, 偶e zachowanie Japoczyk贸w zrozumiae, tylko nasi 'be'...' -> 0\n",
      "95. '@anonymized_account @anonymized_account M贸j york jest z zawodu dyrektorem. \\nSdz, 偶e @anonymized_account potwierdzi....' -> 0\n",
      "96. '@anonymized_account pytam czy robi odprawy po polsku i po angielsku.To 偶e zna angielski to wiem....' -> 0\n",
      "97. '@anonymized_account Boze naprawd?  To ktos z naszego otoczenia? ...' -> 0\n",
      "98. '@anonymized_account mam nadzieje 偶e zostajesz w Wile :)...' -> 0\n",
      "99. '@anonymized_account @anonymized_account @anonymized_account My, Pogo, Lech, lsk, Zagbie....' -> 0\n",
      "100. '@anonymized_account @anonymized_account Odrzuciliscie projekt wic nie.mowcie o prawach...' -> 0\n"
     ]
    }
   ],
   "source": [
    "test_sentences = []\n",
    "with open('hate_test_data.txt', 'r', encoding='utf-8') as f:\n",
    "    test_sentences = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "test_dataset = TestDataset(sentences=test_sentences, tokenizer=tokenizer)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Predykcje\n",
    "all_predictions = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        # Przenie na device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Predykcje\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Zapisz predykcje\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Zapisz do CSV\n",
    "df_predictions = pd.DataFrame({\n",
    "    'prediction': all_predictions\n",
    "})\n",
    "\n",
    "df_predictions.to_csv('pred.csv', index=False, header=False)\n",
    "\n",
    "# Wywietl kilka przykad贸w\n",
    "print(\"\\nPrzykadowe predykcje:\")\n",
    "for i in range(100):\n",
    "    print(f\"{i+1}. '{test_sentences[i][:512]}...' -> {all_predictions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f16717cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ZDANIA ZAKLASYFIKOWANE JAKO KLASA 1:\n",
      "==================================================\n",
      "\n",
      "1. Zdanie #24:\n",
      "   Tekst: @anonymized_account bo wie ju偶 to kraina t, kwa kwa agora za i gg zy tvn\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "2. Zdanie #31:\n",
      "   Tekst: @anonymized_account Jak gram na Orliku to nikt mi wynagrodzenia za to niedaje a biegam i staram si na caego wiec uwa偶am ze to sabe wytumaczenie\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "3. Zdanie #37:\n",
      "   Tekst: @anonymized_account Miego dnia Ci 偶ycz \n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "4. Zdanie #51:\n",
      "   Tekst: RT @anonymized_account @anonymized_account Kiedy ten patologiczny kamca skoczy wreszcie opluwa Polak贸w i Polsk czy niema paragrafu na t hien?\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "5. Zdanie #60:\n",
      "   Tekst: @anonymized_account Obroni wczoraj sam na sam.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "6. Zdanie #61:\n",
      "   Tekst: @anonymized_account Kolejna 偶e jak bura suka\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "7. Zdanie #106:\n",
      "   Tekst: @anonymized_account @anonymized_account Zacze robi za prymasa pajacu?\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "8. Zdanie #107:\n",
      "   Tekst: @anonymized_account To w takim razie trzeba trzyma kciuki za spadek \n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "9. Zdanie #116:\n",
      "   Tekst: @anonymized_account a偶 mi poprawi humor od rana hahahaha\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "10. Zdanie #128:\n",
      "   Tekst: RT @anonymized_account Ej suchajcie co zajebistego wymyliam \\n\\nJaki jest najlepszy argument bonnie z pamitnik贸w wampir贸w? \\n\\nBo nnie\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "11. Zdanie #148:\n",
      "   Tekst: RT @anonymized_account @anonymized_account Ale kamieniczk chuj przytuli po miastowemu?\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "12. Zdanie #170:\n",
      "   Tekst: Jutro po marszu, ale jeszcze kilka dni beda chodzi napani\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "13. Zdanie #183:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account odblokuj @anonymized_account w imi Jarkacza, szak to kawa jest kutasa\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "14. Zdanie #197:\n",
      "   Tekst: Zbojkotowa Mundial w tym dzikim kraju. To tak jak bymy jechali gra w pik w tz. pastwie islamskim.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "15. Zdanie #208:\n",
      "   Tekst: @anonymized_account Bardzo dobrze. Na piechot pisfani. Wybralicie sobie PiS to macie.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "16. Zdanie #230:\n",
      "   Tekst: @anonymized_account I jeszcze jedno pytanie: onjak wiar chodzi w kontekcie tych zabobon贸w?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "17. Zdanie #235:\n",
      "   Tekst: @anonymized_account Tak to jest z pomnikami obroc贸w pedofili\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "18. Zdanie #243:\n",
      "   Tekst: @anonymized_account @anonymized_account A jaki Ty kurwa brzydki jeste...\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "19. Zdanie #248:\n",
      "   Tekst: Ani jednego czowieka z pis nie toleruj, tak mam, co ze mn nie tak chyba\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "20. Zdanie #251:\n",
      "   Tekst: @anonymized_account @anonymized_account \\\"Zadzwocie do rady ministr贸w, 偶e zaraz tam jad\\\"\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "21. Zdanie #252:\n",
      "   Tekst: @anonymized_account udao mi si zda mimo tego, 偶e wszystko wczoraj mi si ju偶 mieszao XD\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "22. Zdanie #260:\n",
      "   Tekst: @anonymized_account A mo偶e wszystko zale偶y od tego dalszego cigu...yyy cignicia ....\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "23. Zdanie #263:\n",
      "   Tekst: RT @anonymized_account Moja odporno to takie dno totalne i chciaabym wreszcie przesta chorowa co chwile\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "24. Zdanie #303:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account A jest miejsce dla takich co za kas z UE finansuj kampani wyborcz swojej partii?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "25. Zdanie #336:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account No to kiedy si mijamy ? No i macie lepszego trenera \n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "26. Zdanie #339:\n",
      "   Tekst: @anonymized_account Aaaaaa  tak czy inaczej we藕 si za the originals bo w czwartek ostatni odcinek kiedykolwiek na kt贸rym odcinku jeste?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "27. Zdanie #348:\n",
      "   Tekst: RT @anonymized_account Okazao si, 偶e pierwszym zwierzciem wystrzelonym w kosmos w 1957r. nie bya 邪泄泻邪, tylko #Morawiecki\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "28. Zdanie #383:\n",
      "   Tekst: RT @anonymized_account @anonymized_account Po slowach ktore wypowiedziales w Lodzi nie powinienes sie pokazywac na uroczystosciach!\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "29. Zdanie #397:\n",
      "   Tekst: @anonymized_account Och, nie bd miarodajna, bo w  ostatnim roku czytaam same starocie.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "30. Zdanie #423:\n",
      "   Tekst: @anonymized_account Od jakiego czasu mam dziwne uczucie suchajc PADa, 偶e jest coraz mniej wiarygodny.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "31. Zdanie #436:\n",
      "   Tekst: @anonymized_account Tak jak powiedzia wczoraj trener - nie wiadomo.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "32. Zdanie #438:\n",
      "   Tekst: \\\"Bomba zostanie zaadowana na specjalny w贸z, tak zwany bombow贸z\\\"\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "33. Zdanie #445:\n",
      "   Tekst: @anonymized_account mam beke z takich ludzi.Niech jeszcze napisze 偶e zarabia za mao w stosunku do Lewandowskiego.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "34. Zdanie #453:\n",
      "   Tekst: W Belfacie zgwacono dwutygodniowego maluszka... na jakim wiecie my 偶yjemy?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "35. Zdanie #456:\n",
      "   Tekst: @anonymized_account @anonymized_account pewnie nie chce sprzedawa i nie ma te偶 przyzwoitych ofert\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "36. Zdanie #462:\n",
      "   Tekst: @anonymized_account @anonymized_account Hehe Cracovia dzis nieznaczy nic w Krakowie ale nawet jak co znaczyla to i tak miaa kompleks Wisy.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "37. Zdanie #468:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account @anonymized_account Gdybymy nie mieli Guilherme, to jak najbardziej.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "38. Zdanie #473:\n",
      "   Tekst: w sumie skd zna moje nazwisko XD pewnie instruktor mu wygada\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "39. Zdanie #481:\n",
      "   Tekst: Ale pogoda dzisiaj to jest jednak okropna\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "40. Zdanie #503:\n",
      "   Tekst: @anonymized_account Mo偶e w Przegldzie tak, lokalne dzienniki, kt贸re maj po 3-4 akredytacje stae, mogyby si czym wykaza.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "41. Zdanie #550:\n",
      "   Tekst: Pita by 偶aosny i przed ujawnieniem romansu.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "42. Zdanie #572:\n",
      "   Tekst: @anonymized_account Zmiecisz si zmiecisz !! Kosmito.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "43. Zdanie #578:\n",
      "   Tekst: @anonymized_account @anonymized_account A id藕cie, najlepiej w pizdu.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "44. Zdanie #585:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account uwa偶aj stara ruro, bo Ci na 偶arcie w sejmie nie wystarczy.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "45. Zdanie #588:\n",
      "   Tekst: @anonymized_account Masz jakie sposoby na zimno? Bo mi tez si robi :(\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "46. Zdanie #589:\n",
      "   Tekst: @anonymized_account @anonymized_account o dzikuje.Szczerze m贸wic nie wiem o co si tak przypierdolie ale spoko :)\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "47. Zdanie #632:\n",
      "   Tekst: @anonymized_account Ale 偶aden z nich nie przebiera si za chuja, 偶eby podkreli swoj waciw rol\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "48. Zdanie #649:\n",
      "   Tekst: RT @anonymized_account @anonymized_account @anonymized_account Nie wiem czy maj wietnie czy nie, wiem kto ich chcia rozpi偶d偶y w drobny mak.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "49. Zdanie #652:\n",
      "   Tekst: @anonymized_account @anonymized_account Ss wszystkimi otworami co tylko da si wessa\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "50. Zdanie #653:\n",
      "   Tekst: Suchajta bo fajne  Blaze Away wg Morcheeba\\nhttps://t.co/PZXhlmHtbs\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "51. Zdanie #683:\n",
      "   Tekst: #Woronicza 17 pose Halicki oburzony za Bolka.Naprawd猫 taki tpy czy tylko udaje idiot?\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "52. Zdanie #710:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account Jak narazie to masz przywidzenia co nie zmienia faktu 偶e cay czas jeste idiot.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "53. Zdanie #716:\n",
      "   Tekst: @anonymized_account Nigdzie nie widz jeszcze \\\"Zima zaskoczya drogowc贸w\\\".\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "54. Zdanie #746:\n",
      "   Tekst: @anonymized_account Messi i Ronaldo maj tak sam rzesze fan贸w i hejter贸w.Wida 偶e jeste lepo w nim zakochany\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "55. Zdanie #748:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account ale ty debil jeste, a o Twoich lajkujcych i dajcych RT nie wspomn.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "56. Zdanie #749:\n",
      "   Tekst: RT @anonymized_account W TYM TYGODNIU NIE MA DYNASTY KDNDNS CZEMU MI TO ROBICIE\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "57. Zdanie #764:\n",
      "   Tekst: @anonymized_account wsp贸czuje rodzinie i znajomym tych trzech dziewczyn bo takie zachowania wiadcz o tym jakim si jest czowiekiem\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "58. Zdanie #773:\n",
      "   Tekst: Heeeheee, prawda to, 100 %, ruskie dziwki @anonymized_account @anonymized_account pewne na 100%\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "59. Zdanie #779:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account Delegat贸w i obserwator贸w te偶 ju偶 podali?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "60. Zdanie #782:\n",
      "   Tekst: Podsumowujc:\\nObciach mierzymy w gmyzach, a \\na poziom utraty kontroli, w kukizach.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "61. Zdanie #784:\n",
      "   Tekst: @anonymized_account Nie bucz, i tak mierdzisz\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "62. Zdanie #836:\n",
      "   Tekst: Kto chce ze mna popisa :(\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "63. Zdanie #842:\n",
      "   Tekst: @anonymized_account Sarapata bya w solarium ?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "64. Zdanie #852:\n",
      "   Tekst: @anonymized_account spraw zainteresowao si BBC ? Niby gdzie ?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "65. Zdanie #863:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account Zgodzisz si ze mn, im mniej polskich mord w spoeczestwie  tym lepiej dla Polski\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "66. Zdanie #880:\n",
      "   Tekst: @anonymized_account @anonymized_account Ona nie myli, bo 偶eby myle trzeba mie m贸zg.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "67. Zdanie #900:\n",
      "   Tekst: @anonymized_account Eh a mogo by 3-0 \n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "68. Zdanie #922:\n",
      "   Tekst: Dziwne. ? Dlaczego tych pienidzy nie wo偶y  do portfela kt贸ry trzyma w rku. Zwyky zodziej !!!\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "69. Zdanie #933:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account niezapomnij napisa w nastpnym tekcie 偶e ka偶dy kiedy umrze.W kocu to prawda\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "70. Zdanie #941:\n",
      "   Tekst: Prowokatorka HGW pokazaa 偶e Polsk i Polak贸w ma  gboko w dupie.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "71. Zdanie #963:\n",
      "   Tekst: @anonymized_account @anonymized_account A id藕cie, najlepiej w pizdu.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "72. Zdanie #973:\n",
      "   Tekst: @anonymized_account @anonymized_account ale czy CHODZILICIE a nie czy CHODZICIE\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "73. Zdanie #983:\n",
      "   Tekst: Jak si nazywa pose, kt贸ry baamuci kobiety na miesiecznicach?\\nChwaliPita.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "74. Zdanie #998:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account kiedy on si dobrze zapowiada ? Jak media pisay 偶e go chce Chelsea ?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "\n",
      "Znaleziono 74 zda zaklasyfikowanych jako klasa 1\n",
      "To stanowi 7.40% wszystkich zda\n",
      "\n",
      "(Pokazuj tylko pierwsze 20 zda)\n",
      "\n",
      "1. @anonymized_account bo wie ju偶 to kraina t, kwa kwa agora za i gg zy tvn\n",
      "\n",
      "2. @anonymized_account Jak gram na Orliku to nikt mi wynagrodzenia za to niedaje a biegam i staram si na caego wiec uwa偶am ze to sabe wytumaczenie\n",
      "\n",
      "3. @anonymized_account Miego dnia Ci 偶ycz \n",
      "\n",
      "4. RT @anonymized_account @anonymized_account Kiedy ten patologiczny kamca skoczy wreszcie opluwa Polak贸w i Polsk czy niema paragrafu na t hien?\n",
      "\n",
      "5. @anonymized_account Obroni wczoraj sam na sam.\n",
      "\n",
      "6. @anonymized_account Kolejna 偶e jak bura suka\n",
      "\n",
      "7. @anonymized_account @anonymized_account Zacze robi za prymasa pajacu?\n",
      "\n",
      "8. @anonymized_account To w takim razie trzeba trzyma kciuki za spadek \n",
      "\n",
      "9. @anonymized_account a偶 mi poprawi humor od rana hahahaha\n",
      "\n",
      "10. RT @anonymized_account Ej suchajcie co zajebistego wymyliam \\n\\nJaki jest najlepszy argument bonnie z pamitnik贸w wampir贸w? \\n\\nBo nnie\n",
      "\n",
      "11. RT @anonymized_account @anonymized_account Ale kamieniczk chuj przytuli po miastowemu?\n",
      "\n",
      "12. Jutro po marszu, ale jeszcze kilka dni beda chodzi napani\n",
      "\n",
      "13. @anonymized_account @anonymized_account @anonymized_account odblokuj @anonymized_account w imi Jarkacza, szak to kawa jest kutasa\n",
      "\n",
      "14. Zbojkotowa Mundial w tym dzikim kraju. To tak jak bymy jechali gra w pik w tz. pastwie islamskim.\n",
      "\n",
      "15. @anonymized_account Bardzo dobrze. Na piechot pisfani. Wybralicie sobie PiS to macie.\n",
      "\n",
      "16. @anonymized_account I jeszcze jedno pytanie: onjak wiar chodzi w kontekcie tych zabobon贸w?\n",
      "\n",
      "17. @anonymized_account Tak to jest z pomnikami obroc贸w pedofili\n",
      "\n",
      "18. @anonymized_account @anonymized_account A jaki Ty kurwa brzydki jeste...\n",
      "\n",
      "19. Ani jednego czowieka z pis nie toleruj, tak mam, co ze mn nie tak chyba\n",
      "\n",
      "20. @anonymized_account @anonymized_account \\\"Zadzwocie do rady ministr贸w, 偶e zaraz tam jad\\\"\n"
     ]
    }
   ],
   "source": [
    "# Wypisz zdania z przewidywan klas 1\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ZDANIA ZAKLASYFIKOWANE JAKO KLASA 1:\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Znajd藕 indeksy gdzie przewidywania = 1\n",
    "predicted_class_1_indices = [i for i, pred in enumerate(all_predictions) if pred == 1]\n",
    "\n",
    "# Wypisz zdania\n",
    "for i, idx in enumerate(predicted_class_1_indices):\n",
    "    print(f\"{i+1}. Zdanie #{idx}:\")\n",
    "    print(f\"   Tekst: {X_val[idx]}\")\n",
    "    print(f\"   Rzeczywista etykieta: {all_labels[idx]}\")\n",
    "    print(f\"   Przewidywana etykieta: {all_predictions[idx]}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Podsumowanie\n",
    "print(f\"\\nZnaleziono {len(predicted_class_1_indices)} zda zaklasyfikowanych jako klasa 1\")\n",
    "print(f\"To stanowi {len(predicted_class_1_indices)/len(all_predictions)*100:.2f}% wszystkich zda\")\n",
    "\n",
    "# Opcjonalnie: wypisz tylko pierwsze N zda jeli jest ich du偶o\n",
    "MAX_DISPLAY = 20\n",
    "if len(predicted_class_1_indices) > MAX_DISPLAY:\n",
    "    print(f\"\\n(Pokazuj tylko pierwsze {MAX_DISPLAY} zda)\")\n",
    "    for i in range(MAX_DISPLAY):\n",
    "        idx = predicted_class_1_indices[i]\n",
    "        print(f\"\\n{i+1}. {X_val[idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
