{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2feb905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/tscratch/people/plgmilosz03/conda-envs/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/net/tscratch/people/plgmilosz03/conda-envs/env/lib/python3.12/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/net/tscratch/people/plgmilosz03/conda-envs/env/lib/python3.12/site-packages/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/net/tscratch/people/plgmilosz03/conda-envs/env/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/net/tscratch/people/plgmilosz03/conda-envs/env/lib/python3.12/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import re\n",
    "from peft import LoraConfig, TaskType, PeftModel, get_peft_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    CLIPModel,\n",
    "    CLIPProcessor,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560354e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1592c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd3292aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, num_labels=2, output_attentions=False, output_hidden_states=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60edd25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, sentences, labels, tokenizer, max_length=512):\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = emoji.replace_emoji(text, replace='')\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'^RT\\s+', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "        text = re.sub(r'www\\.\\S+', '', text)\n",
    "        text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "        text = text.strip()\n",
    "                \n",
    "        return text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.preprocess_text(str(self.sentences[idx]))\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,           \n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f408dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dla mnie faworytem do tytułu będzie Cracovia. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@anonymized_account @anonymized_account Brawo ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@anonymized_account @anonymized_account Super,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@anonymized_account @anonymized_account Musi. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Odrzut natychmiastowy, kwaśna mina, mam problem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  Dla mnie faworytem do tytułu będzie Cracovia. ...      0\n",
       "1  @anonymized_account @anonymized_account Brawo ...      0\n",
       "2  @anonymized_account @anonymized_account Super,...      0\n",
       "3  @anonymized_account @anonymized_account Musi. ...      0\n",
       "4    Odrzut natychmiastowy, kwaśna mina, mam problem      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"hate_train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e5daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['sentence'].tolist()\n",
    "labels = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f555506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozkład klas przed oversampling:\n",
      "label\n",
      "0    9190\n",
      "1     851\n",
      "Name: count, dtype: int64\n",
      "Procent klasy 1: 8.48%\n",
      "Całkowita liczba próbek: 10041\n",
      "\n",
      "Rozkład klas po oversampling:\n",
      "label\n",
      "0    9190\n",
      "1    4255\n",
      "Name: count, dtype: int64\n",
      "Procent klasy 1: 31.65%\n",
      "Całkowita liczba próbek: 13445\n",
      "\n",
      "Po train_test_split:\n",
      "Train: 10756 próbek\n",
      "Val: 2689 próbek\n"
     ]
    }
   ],
   "source": [
    "print(\"Rozkład klas przed oversampling:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"Procent klasy 1: {(df['label']==1).sum() / len(df) * 100:.2f}%\")\n",
    "print(f\"Całkowita liczba próbek: {len(df)}\")\n",
    "\n",
    "multiplier = 5\n",
    "\n",
    "positive_samples = df[df['label'] == 1]\n",
    "\n",
    "duplicated_positives = pd.concat([positive_samples] * (multiplier - 1), ignore_index=True)\n",
    "\n",
    "df_oversampled = pd.concat([df, duplicated_positives], ignore_index=True)\n",
    "\n",
    "df_oversampled = df_oversampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nRozkład klas po oversampling:\")\n",
    "print(df_oversampled['label'].value_counts())\n",
    "print(f\"Procent klasy 1: {(df_oversampled['label']==1).sum() / len(df_oversampled) * 100:.2f}%\")\n",
    "print(f\"Całkowita liczba próbek: {len(df_oversampled)}\")\n",
    "\n",
    "sentences = df_oversampled['sentence'].tolist()\n",
    "labels = df_oversampled['label'].tolist()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    sentences, \n",
    "    labels, \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"\\nPo train_test_split:\")\n",
    "print(f\"Train: {len(X_train)} próbek\")\n",
    "print(f\"Val: {len(X_val)} próbek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb933130",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HateSpeechDataset(sentences=X_train, labels=y_train, tokenizer=tokenizer)\n",
    "eval_dataset = HateSpeechDataset(sentences=X_val, labels=y_val, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "009df412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 184,423,682\n",
      "DebertaV2ForSequenceClassification(\n",
      "  (deberta): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (pooler): ContextPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): StableDropout()\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): StableDropout()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c879c061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DebertaV2ForSequenceClassification(\n",
      "      (deberta): DebertaV2Model(\n",
      "        (embeddings): DebertaV2Embeddings(\n",
      "          (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "          (dropout): StableDropout()\n",
      "        )\n",
      "        (encoder): DebertaV2Encoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x DebertaV2Layer(\n",
      "              (attention): DebertaV2Attention(\n",
      "                (self): DisentangledSelfAttention(\n",
      "                  (query_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (key_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (value_proj): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (pos_dropout): StableDropout()\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "                (output): DebertaV2SelfOutput(\n",
      "                  (dense): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                  )\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "              )\n",
      "              (intermediate): DebertaV2Intermediate(\n",
      "                (dense): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=3072, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DebertaV2Output(\n",
      "                (dense): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=3072, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (rel_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (pooler): ContextPooler(\n",
      "        (dense): lora.Linear(\n",
      "          (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora_dropout): ModuleDict(\n",
      "            (default): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (lora_A): ModuleDict(\n",
      "            (default): Linear(in_features=768, out_features=16, bias=False)\n",
      "          )\n",
      "          (lora_B): ModuleDict(\n",
      "            (default): Linear(in_features=16, out_features=768, bias=False)\n",
      "          )\n",
      "          (lora_embedding_A): ParameterDict()\n",
      "          (lora_embedding_B): ParameterDict()\n",
      "        )\n",
      "        (dropout): StableDropout()\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"query_proj\", \n",
    "        \"key_proj\", \n",
    "        \"value_proj\", \n",
    "        \"dense\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_results\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=25,\n",
    "    per_device_eval_batch_size=10,\n",
    "    logging_dir=\"./lora_logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-4,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f7d823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 2,680,322\n"
     ]
    }
   ],
   "source": [
    "# Print number of trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8194453b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8620' max='8620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8620/8620 56:56, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.577400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.482400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.415300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.345700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.304700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.255100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.134800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.159900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.099000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.102300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.107200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.081900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.070200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.056700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.043200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.025400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.018100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.006200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./lora_results/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-2000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-2500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-3000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-3500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-4000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-4500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-5000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-5500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory ./lora_results/checkpoint-6000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8620, training_loss=0.06875721449929174, metrics={'train_runtime': 3417.5301, 'train_samples_per_second': 62.946, 'train_steps_per_second': 2.522, 'total_flos': 5.837275244101632e+16, 'train_loss': 0.06875721449929174, 'epoch': 20.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c02e2fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./lora_adapters/tokenizer_config.json',\n",
       " './lora_adapters/special_tokens_map.json',\n",
       " './lora_adapters/spm.model',\n",
       " './lora_adapters/added_tokens.json',\n",
       " './lora_adapters/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./lora_adapters\")\n",
    "tokenizer.save_pretrained(\"./lora_adapters\")\n",
    "\n",
    "# base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"microsoft/deberta-v3-base\",\n",
    "#     num_labels=2,\n",
    "#     output_attentions=False,\n",
    "#     output_hidden_states=False,\n",
    "# )\n",
    "\n",
    "# model = PeftModel.from_pretrained(base_model, \"./lora_adapters\")\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Załaduj tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"./lora_adapters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76853560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.9803\n",
      "Precision:       0.9414\n",
      "Recall:          1.0000\n",
      "F1-Score:        0.9698\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Evaluate without gradient computation\n",
    "with torch.no_grad():\n",
    "    for batch in eval_dataloader:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "        # Store predictions and labels\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Model Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision:       {precision:.4f}\")\n",
    "print(f\"Recall:          {recall:.4f}\")\n",
    "print(f\"F1-Score:        {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a02c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, max_length=256):\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        text = emoji.replace_emoji(text, replace='')\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "        text = re.sub(r'^RT\\s+', '', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "        text = re.sub(r'www\\.\\S+', '', text)\n",
    "        text = text.replace('\\n', ' ').replace('\\t', ' ')\n",
    "        text = ' '.join(text.split())\n",
    "        text = text.strip()\n",
    "                \n",
    "        return text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.preprocess_text(str(self.sentences[idx]))\n",
    "        encoding = self.tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,           \n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd5f010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Przykładowe predykcje:\n",
      "1. '@anonymized_account Spoko, jak im Duda z Morawieckim zamówią po pięć piw to wszystko będzie ok....' -> 0\n",
      "2. '@anonymized_account @anonymized_account Ale on tu nie miał szans jej zagrania, a ta 'proba' to czysta prowizorka....' -> 0\n",
      "3. '@anonymized_account No czy Prezes nie miał racji, mówiąc,ze to są zdradzieckie mordy? No czy nie miał racji?😁😁...' -> 0\n",
      "4. '@anonymized_account @anonymized_account Przecież to nawet nie jest przewrotka 😂...' -> 0\n",
      "5. '@anonymized_account @anonymized_account Owszem podatki tak. Ale nie w takich okolicznościach. Czemu Małysza odpalili z teamu Orlen?...' -> 0\n",
      "6. '@anonymized_account @anonymized_account skąd wiesz jaki Skendija ma budżet skoro mówisz że jest bogatsza ? Tylko dwóch zawodników ponoć dobrze zarabia....' -> 0\n",
      "7. 'Z tego, co widzę, to kibice Widzewa mają szczęście, że trwa mundial. Dzięki temu ogólnopolska szydera jest tylko z Argentyny i Messiego....' -> 0\n",
      "8. '@anonymized_account @anonymized_account @anonymized_account Na utrzymanie własnej armii 2% PKB, tyle że teraz to jedna wielka ściema...' -> 0\n",
      "9. 'Przypomnijcie mi ze muszę jeszcze suszarkę spakować...' -> 0\n",
      "10. 'Czy Adam już nie żyje? Jeśli tak, to jak rozwiązali jego wątek?  #nadobreinazłe...' -> 0\n",
      "11. '@anonymized_account Nie zmieścił się w kadrze to dostał wolne, bo przecież teraz będzie przerwa na kadrę....' -> 0\n",
      "12. 'Jajka na miękko czy na twardo? Jeśli jecie jajka ofc...' -> 0\n",
      "13. '@anonymized_account @anonymized_account Ty wiesz lepiej. Ja wiem, że nawet wiceprezydentem nie będziesz na 100%...' -> 0\n",
      "14. '@anonymized_account @anonymized_account Kto mieczem wojuje, ten od pochwy ginie...' -> 0\n",
      "15. '@anonymized_account Może się mylę, ale tego Szwajcara po lewej to w ogóle mógł dobrze nie widzieć :)...' -> 0\n",
      "16. '@anonymized_account Zaraz Wisła pewnie dopłaci Legii, żeby tylko ogłosili, że kupili go za 2mln euro 😂...' -> 0\n",
      "17. '@anonymized_account Dopiero teraz jest sterylne dzięki maseczce....' -> 0\n",
      "18. '@anonymized_account Nie mam informacji na ten temat....' -> 0\n",
      "19. '@anonymized_account @anonymized_account @anonymized_account Absolutnie nie o te kryteria chodzi....' -> 0\n",
      "20. '@anonymized_account Równie praktyczne, jak jazda na rowerze górskim w za dużych o rozmiar szpilkach dwunastkach....' -> 0\n",
      "21. 'Nowa dziewczyna Liama ma serio 18 lat?...' -> 0\n",
      "22. '@anonymized_account @anonymized_account @anonymized_account W takim razie sie zapedzilem, i tak nie daje banów, nawet botom...' -> 0\n",
      "23. '@anonymized_account wieczna zdrada, nie zdrada, trzeba rozmawiać, pierdolenie od rzeczy...' -> 0\n",
      "24. '@anonymized_account na Westerplatte 13 jest sklep Lego...' -> 0\n",
      "25. '@anonymized_account Tej szmaty się nie komentuje...' -> 1\n",
      "26. '@anonymized_account A ty taki dziennikarz, że nie wiesz co oznacza znak zapytania 😂...' -> 0\n",
      "27. '@anonymized_account @anonymized_account I dzięki temu mamy dziś cesarzową..😁😁😁. Gorąco pozdrawiam!😁😁😁...' -> 0\n",
      "28. 'Jutro media współczesne, wf i retoryka, sounds gr8...' -> 0\n",
      "29. '@anonymized_account potem nie wytykaj innym że Polacy to mistrzowie w narzekaniu...' -> 0\n",
      "30. '@anonymized_account Mogłem, mogłem, mogłem coś zrobić zamiast ciągle walić konia...' -> 0\n",
      "31. '@anonymized_account @anonymized_account To Pani Krysia jest Twoją Ojczyzną?...' -> 0\n",
      "32. '@anonymized_account Dokładnie, pisdzielstwo nie ma prawa rozpierdalać systemu,  sądownictwa nie mając większości...' -> 1\n",
      "33. '@anonymized_account Nie wiedziałam, że jest takie państwo jak HINY....' -> 0\n",
      "34. 'Robert wszedł do łazienki a to znaczy ze mogę sobie pomarzyć o prysznicu przynajmniej przez jakieś 1,5 godziny jeszcze ugh...' -> 0\n",
      "35. 'Myślałem, że ten mecz będzie dla Brazylijczyków trudniejszy....' -> 0\n",
      "36. '@anonymized_account Zgadzam się, aczkolwiek nie ma takiej mocy ofensywnej, więc może być to mecz na 1:0....' -> 0\n",
      "37. '@anonymized_account Mi chodzi o to, że wg artykułów sprawa też jest do rozwiązania kontraktu. Powody oczywiście są inne i widzę różnice....' -> 0\n",
      "38. '@anonymized_account Adrian Juda, figurant WSI i  lobby żydowskiego...' -> 1\n",
      "39. '@anonymized_account Pewnie są na 180 km...' -> 0\n",
      "40. '@anonymized_account Nieprawda. Normalnie tak się nie \\\"tnie\\\"....' -> 0\n",
      "41. '@anonymized_account Już mamy się śmiać ?...' -> 0\n",
      "42. '@anonymized_account Chłop z największymi kompleksami pisze coś o kompleksach u innych 😂...' -> 0\n",
      "43. '@anonymized_account Michniewicza czy Matusiaka też lubisz, ale to że handlowali meczami to już możesz znieść 😂...' -> 0\n",
      "44. '@anonymized_account @anonymized_account @anonymized_account w nagrode wypij mój syrop! :D...' -> 0\n",
      "45. 'Coś się dzieje atak botów z każdej strony...' -> 0\n",
      "46. 'A na żywo brzmi jak niebo...' -> 0\n",
      "47. '@anonymized_account Bez problemu Czechy i Litwa nas wyprzedziły mimo tego historycznego wzrostu...' -> 0\n",
      "48. '@anonymized_account warunkowo na pewno by dostał pozwolenie.To przecież III liga...' -> 0\n",
      "49. '@anonymized_account @anonymized_account @anonymized_account @anonymized_account Chętnie zrobie Wam syrop hawajski z ananasem :D pójdę do szefa z tą propozycją 😂...' -> 0\n",
      "50. '@anonymized_account @anonymized_account po fryzjerze zawsze się dobrze wygląda. Chwilę....' -> 0\n",
      "51. '@anonymized_account Za takie akcje to tylko legia ma wyłączność u polskich sędziów na karne także Jaga Luz 😜...' -> 0\n",
      "52. '@anonymized_account Widać ludziom w Sączu tak pasuje. Ja bym mu w gębę napluł...' -> 1\n",
      "53. '@anonymized_account Że się ten operator nie wyłożył......' -> 0\n",
      "54. 'na ból gardła najlepsza wódeczka...' -> 0\n",
      "55. '@anonymized_account Od sędziów z postępowaniami dyscyplinarnymi kandydującymi do KRS...' -> 0\n",
      "56. '@anonymized_account @anonymized_account mamy brać kogoś bo inni biorą ?...' -> 0\n",
      "57. '@anonymized_account Droga p.Kamilko! Leczyć się . Leczyć póki czas😁😁...' -> 0\n",
      "58. '@anonymized_account powinnaś odpowiedzieć za działanie na szkodę Polski i obywateli...' -> 0\n",
      "59. 'RT @anonymized_account @anonymized_account powinnaś odpowiedzieć za działanie na szkodę Polski i obywateli...' -> 0\n",
      "60. '@anonymized_account Awww dziękuje z całego serduszka...' -> 0\n",
      "61. '@anonymized_account Dubaj wzorem, czyli jednak islam, salam alejkum Ciemkiewicz...' -> 1\n",
      "62. '@anonymized_account @anonymized_account @anonymized_account Najbardziej to on jest wolny od mózgu....' -> 1\n",
      "63. 'muszę podjechać dzisiaj do mechanika...' -> 0\n",
      "64. '@anonymized_account 2/2 a później się zastanawiaj po co Wiśle akademia i wychwalaj akademie Legii i ich stawianie na młodzież...' -> 0\n",
      "65. '@anonymized_account   Półgłówek Wieliński, wymyślił sobie półautorytaryzm!...' -> 0\n",
      "66. 'RT @anonymized_account @anonymized_account   Półgłówek Wieliński, wymyślił sobie półautorytaryzm!...' -> 0\n",
      "67. '@anonymized_account Ale najpierw Sadurskiego do Australii...' -> 0\n",
      "68. '@anonymized_account A w kościółku już byłeś?...' -> 0\n",
      "69. '@anonymized_account A wy macie? Razem z Piotrowicz en i resztą?...' -> 0\n",
      "70. 'Dzisiaj musiałam pracować w grupie z chłopakami z roku i myślałam ze wyjdę z siebie i z tej sali I mean jak można być tak infantylnym...' -> 0\n",
      "71. 'Co za bełkot ten halicki.  #Woronicza17...' -> 0\n",
      "72. 'niech mnie ktoś zabierze w góry...' -> 0\n",
      "73. 'Decyzją spikera brytyjskiej Izby Gmin noszenie krawatów przez posłów i reporterów podczas posiedzeń parlamentu nie będzie już obowiązkowe 👔...' -> 0\n",
      "74. '@anonymized_account @anonymized_account Podobnie, jak mój w odniesieniu do męskiej kreatywności 👍😉...' -> 0\n",
      "75. '@anonymized_account @anonymized_account @anonymized_account @anonymized_account Ja nie wiem, nie dowiadywałem się :)...' -> 0\n",
      "76. '@anonymized_account Ciebie też nie ma, wikarku...' -> 0\n",
      "77. '@anonymized_account @anonymized_account Sprawdzam na flightradarze, samolot \\\"już\\\" wleciał do Polski....' -> 0\n",
      "78. 'Z kolei Luka Gugeszaszwili w pięknym stylu wyjął karnego w meczu Finlandia - Gruzja (1:2)....' -> 0\n",
      "79. '@anonymized_account @anonymized_account jak na nokia 3310 to spoko :)...' -> 0\n",
      "80. '@anonymized_account Chyba, że tradycyjnie wylosujemy Zakaukazie i mecze będą o 16 naszego czasu 😂...' -> 0\n",
      "81. 'Mężczyźni siedzący przed gabinetem u laryngologa nie czekają na wizytę; oni walczą o przetrwanie gatunku....' -> 0\n",
      "82. '@anonymized_account a ja założę fitbloga 😂😂😂...' -> 0\n",
      "83. '@anonymized_account czyli dla Legii też nie ma miejsca @anonymized_account...' -> 0\n",
      "84. '@anonymized_account @anonymized_account @anonymized_account Podstawowe zadanie każdego księdza to narzucanie wiary....' -> 0\n",
      "85. '@anonymized_account Mam podobny i mówili ze to ciemny blond chociaż rozjaśniłam i teraz faktycznie jestem blondynka...' -> 0\n",
      "86. '@anonymized_account @anonymized_account Na szczęście @anonymized_account i  jego partia znikną  wkrótce \\nze sceny politycznej. Brawo!...' -> 0\n",
      "87. '@anonymized_account Otworzysz następny? \\nPrzyjdzie na ciebie pora popaprańcu. TS czeka....' -> 0\n",
      "88. '@anonymized_account @anonymized_account @anonymized_account Spodobały mi piosenki które śpiewają...' -> 0\n",
      "89. 'Fav albo rt to zrobię Wam indy!!! Nie wiem czy wszystkim, zależy kogo będę kojarzyć 🔥🔥🔥...' -> 0\n",
      "90. '@anonymized_account @anonymized_account @anonymized_account I wojna Gadowskiego z psychiatrą....' -> 0\n",
      "91. 'youngblood to piosenka moich wakacji...' -> 0\n",
      "92. '@anonymized_account ostatnio pewnie chciałeś usłyszeć od Pazdana jacy to kibice Wisły źli a tu dupa...' -> 0\n",
      "93. '@anonymized_account Dziecko teraz walczy o życie bo przecież mam takie straszne obrażenia...' -> 0\n",
      "94. '@anonymized_account Dokładnie, a czytam w niektórych miejscach, że zachowanie Japończyków zrozumiałe, tylko nasi 'be'...' -> 0\n",
      "95. '@anonymized_account @anonymized_account Mój york jest z zawodu dyrektorem. \\nSądzę, że @anonymized_account potwierdzi....' -> 0\n",
      "96. '@anonymized_account pytam czy robi odprawy po polsku i po angielsku.To że zna angielski to wiem....' -> 0\n",
      "97. '@anonymized_account Boze naprawdę? 😮 To ktos z naszego otoczenia? 🤔...' -> 0\n",
      "98. '@anonymized_account mam nadzieje że zostajesz w Wiśle :)...' -> 0\n",
      "99. '@anonymized_account @anonymized_account @anonymized_account My, Pogoń, Lech, Śląsk, Zagłębie....' -> 0\n",
      "100. '@anonymized_account @anonymized_account Odrzuciliscie projekt więc nie.mowcie o prawach...' -> 0\n"
     ]
    }
   ],
   "source": [
    "test_sentences = []\n",
    "with open('hate_test_data.txt', 'r', encoding='utf-8') as f:\n",
    "    test_sentences = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "test_dataset = TestDataset(sentences=test_sentences, tokenizer=tokenizer)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Predykcje\n",
    "all_predictions = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        # Przenieś na device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        \n",
    "        # Predykcje\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        # Zapisz predykcje\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Zapisz do CSV\n",
    "df_predictions = pd.DataFrame({\n",
    "    'prediction': all_predictions\n",
    "})\n",
    "\n",
    "df_predictions.to_csv('pred.csv', index=False, header=False)\n",
    "\n",
    "# Wyświetl kilka przykładów\n",
    "print(\"\\nPrzykładowe predykcje:\")\n",
    "for i in range(100):\n",
    "    print(f\"{i+1}. '{test_sentences[i][:512]}...' -> {all_predictions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f16717cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ZDANIA ZAKLASYFIKOWANE JAKO KLASA 1:\n",
      "==================================================\n",
      "\n",
      "1. Zdanie #24:\n",
      "   Tekst: @anonymized_account bo wie już to kraina tą, kwa kwa agora zła i gęgę zły tvn\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "2. Zdanie #31:\n",
      "   Tekst: @anonymized_account Jak gram na Orliku to nikt mi wynagrodzenia za to niedaje a biegam i staram się na całego wiec uważam ze to słabe wytłumaczenie\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "3. Zdanie #37:\n",
      "   Tekst: @anonymized_account Miłego dnia Ci życzę 🌼\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "4. Zdanie #51:\n",
      "   Tekst: RT @anonymized_account @anonymized_account Kiedy ten patologiczny kłamca skończy wreszcie opluwać Polaków i Polskę czy niema paragrafu na tę hienę?\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "5. Zdanie #60:\n",
      "   Tekst: @anonymized_account Obronił wczoraj sam na sam.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "6. Zdanie #61:\n",
      "   Tekst: @anonymized_account Kolejna łże jak bura suka\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "7. Zdanie #106:\n",
      "   Tekst: @anonymized_account @anonymized_account Zacząłeś robić za prymasa pajacu?\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "8. Zdanie #107:\n",
      "   Tekst: @anonymized_account To w takim razie trzeba trzymać kciuki za spadek 😊\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "9. Zdanie #116:\n",
      "   Tekst: @anonymized_account aż mi poprawił humor od rana hahahaha\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "10. Zdanie #128:\n",
      "   Tekst: RT @anonymized_account Ej słuchajcie co zajebistego wymyśliłam \\n\\nJaki jest najlepszy argument bonnie z pamiętników wampirów? \\n\\nBo nnie\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "11. Zdanie #148:\n",
      "   Tekst: RT @anonymized_account @anonymized_account Ale kamieniczkę chuj przytulił po miastowemu?\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "12. Zdanie #170:\n",
      "   Tekst: Jutro po marszu, ale jeszcze kilka dni beda chodzić naćpani\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "13. Zdanie #183:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account odblokuj @anonymized_account w imię Jarkacza, szak to kawał jest kutasa\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "14. Zdanie #197:\n",
      "   Tekst: Zbojkotować Mundial w tym dzikim kraju. To tak jak byśmy jechali grać w piłkę w tz. państwie islamskim.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "15. Zdanie #208:\n",
      "   Tekst: @anonymized_account Bardzo dobrze. Na piechotę pisfani. Wybraliście sobie PiS to macie.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "16. Zdanie #230:\n",
      "   Tekst: @anonymized_account I jeszcze jedno pytanie: onjaką wiarę chodzi w kontekście tych zabobonów?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "17. Zdanie #235:\n",
      "   Tekst: @anonymized_account Tak to jest z pomnikami obrońców pedofili\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "18. Zdanie #243:\n",
      "   Tekst: @anonymized_account @anonymized_account A jaki Ty kurwa brzydki jesteś...\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "19. Zdanie #248:\n",
      "   Tekst: Ani jednego człowieka z pis nie toleruję, tak mam, coś ze mną nie tak chyba\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "20. Zdanie #251:\n",
      "   Tekst: @anonymized_account @anonymized_account \\\"Zadzwońcie do rady ministrów, że zaraz tam jadę\\\"\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "21. Zdanie #252:\n",
      "   Tekst: @anonymized_account udało mi się zdać mimo tego, że wszystko wczoraj mi się już mieszało XD\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "22. Zdanie #260:\n",
      "   Tekst: @anonymized_account A może wszystko zależy od tego dalszego ciągu...yyy ciągnięcia ....\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "23. Zdanie #263:\n",
      "   Tekst: RT @anonymized_account Moja odporność to takie dno totalne i chciałabym wreszcie przestać chorować co chwile\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "24. Zdanie #303:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account A jest miejsce dla takich co za kasę z UE finansują kampanię wyborczą swojej partii?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "25. Zdanie #336:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account No to kiedy się mijamy ? No i macie lepszego trenera 😂👏\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "26. Zdanie #339:\n",
      "   Tekst: @anonymized_account Aaaaaa 😂 tak czy inaczej weź się za the originals bo w czwartek ostatni odcinek kiedykolwiek 💔na którym odcinku jesteś?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "27. Zdanie #348:\n",
      "   Tekst: RT @anonymized_account Okazało się, że pierwszym zwierzęciem wystrzelonym w kosmos w 1957r. nie była Лайка, tylko #Morawiecki\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "28. Zdanie #383:\n",
      "   Tekst: RT @anonymized_account @anonymized_account Po slowach ktore wypowiedziales w Lodzi nie powinienes sie pokazywac na uroczystosciach!\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "29. Zdanie #397:\n",
      "   Tekst: @anonymized_account Och, nie będę miarodajna, bo w  ostatnim roku czytałam same starocie.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "30. Zdanie #423:\n",
      "   Tekst: @anonymized_account Od jakiegoś czasu mam dziwne uczucie słuchając PADa, że jest coraz mniej wiarygodny.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "31. Zdanie #436:\n",
      "   Tekst: @anonymized_account Tak jak powiedział wczoraj trener - nie wiadomo.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "32. Zdanie #438:\n",
      "   Tekst: \\\"Bomba zostanie załadowana na specjalny wóz, tak zwany bombowóz\\\"\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "33. Zdanie #445:\n",
      "   Tekst: @anonymized_account mam beke z takich ludzi.Niech jeszcze napisze że zarabia za mało w stosunku do Lewandowskiego.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "34. Zdanie #453:\n",
      "   Tekst: W Belfaście zgwałcono dwutygodniowego maluszka... na jakim świecie my żyjemy?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "35. Zdanie #456:\n",
      "   Tekst: @anonymized_account @anonymized_account pewnie nie chce sprzedawać i nie ma też przyzwoitych ofert\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "36. Zdanie #462:\n",
      "   Tekst: @anonymized_account @anonymized_account Hehe Cracovia dzis nieznaczy nic w Krakowie ale nawet jak coś znaczyla to i tak miała kompleks Wisły.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "37. Zdanie #468:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account @anonymized_account Gdybyśmy nie mieli Guilherme, to jak najbardziej.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "38. Zdanie #473:\n",
      "   Tekst: w sumie skąd znał moje nazwisko XD pewnie instruktor mu wygadał\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "39. Zdanie #481:\n",
      "   Tekst: Ale pogoda dzisiaj to jest jednak okropna\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "40. Zdanie #503:\n",
      "   Tekst: @anonymized_account Może w Przeglądzie tak, lokalne dzienniki, które mają po 3-4 akredytacje stałe, mogłyby się czymś wykazać.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "41. Zdanie #550:\n",
      "   Tekst: Pięta był żałosny i przed ujawnieniem romansu.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "42. Zdanie #572:\n",
      "   Tekst: @anonymized_account Zmieścisz się zmieścisz !! Kosmito.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "43. Zdanie #578:\n",
      "   Tekst: @anonymized_account @anonymized_account A idźcie, najlepiej w pizdu.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "44. Zdanie #585:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account uważaj stara ruro, bo Ci na żarcie w sejmie nie wystarczy.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "45. Zdanie #588:\n",
      "   Tekst: @anonymized_account Masz jakieś sposoby na zimno? Bo mi tez się robi :(\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "46. Zdanie #589:\n",
      "   Tekst: @anonymized_account @anonymized_account o dziękuje.Szczerze mówiąc nie wiem o co się tak przypierdoliłeś ale spoko :)\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "47. Zdanie #632:\n",
      "   Tekst: @anonymized_account Ale żaden z nich nie przebiera się za chuja, żeby podkreślić swoją właściwą rolę\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "48. Zdanie #649:\n",
      "   Tekst: RT @anonymized_account @anonymized_account @anonymized_account Nie wiem czy mają świetnie czy nie, wiem kto ich chciał rozpiżdżyć w drobny mak.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "49. Zdanie #652:\n",
      "   Tekst: @anonymized_account @anonymized_account Ssą wszystkimi otworami co tylko da się wessać\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "50. Zdanie #653:\n",
      "   Tekst: Słuchajta bo fajne  Blaze Away wg Morcheeba\\nhttps://t.co/PZXhlmHtbs\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "51. Zdanie #683:\n",
      "   Tekst: #Woronicza 17 poseł Halicki oburzony za Bolka.Naprawdè taki tępy czy tylko udaje idiotę?\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "52. Zdanie #710:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account Jak narazie to masz przywidzenia co nie zmienia faktu że cały czas jesteś idiotą.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "53. Zdanie #716:\n",
      "   Tekst: @anonymized_account Nigdzie nie widzę jeszcze \\\"Zima zaskoczyła drogowców\\\".\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "54. Zdanie #746:\n",
      "   Tekst: @anonymized_account Messi i Ronaldo mają taką samą rzesze fanów i hejterów.Widać że jesteś ślepo w nim zakochany\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "55. Zdanie #748:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account ale ty debil jesteś, a o Twoich lajkujących i dających RT nie wspomnę.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "56. Zdanie #749:\n",
      "   Tekst: RT @anonymized_account W TYM TYGODNIU NIE MA DYNASTY KDNDNS CZEMU MI TO ROBICIE\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "57. Zdanie #764:\n",
      "   Tekst: @anonymized_account współczuje rodzinie i znajomym tych trzech dziewczyn bo takie zachowania świadczą o tym jakim się jest człowiekiem\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "58. Zdanie #773:\n",
      "   Tekst: Heeeheee, prawda to, 100 %, ruskie dziwki @anonymized_account @anonymized_account pewne na 100%\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "59. Zdanie #779:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account Delegatów i obserwatorów też już podali?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "60. Zdanie #782:\n",
      "   Tekst: Podsumowując:\\nObciach mierzymy w gmyzach, a \\na poziom utraty kontroli, w kukizach.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "61. Zdanie #784:\n",
      "   Tekst: @anonymized_account Nie bucz, i tak śmierdzisz\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "62. Zdanie #836:\n",
      "   Tekst: Kto chce ze mna popisać :(\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "63. Zdanie #842:\n",
      "   Tekst: @anonymized_account Sarapata była w solarium ?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "64. Zdanie #852:\n",
      "   Tekst: @anonymized_account sprawą zainteresowało się BBC ? Niby gdzie ?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "65. Zdanie #863:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account Zgodzisz się ze mną, im mniej polskich mord w społeczeństwie  tym lepiej dla Polski\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "66. Zdanie #880:\n",
      "   Tekst: @anonymized_account @anonymized_account Ona nie myśli, bo żeby myśleć trzeba mieć mózg.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "67. Zdanie #900:\n",
      "   Tekst: @anonymized_account Eh a mogło być 3-0 😁\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "68. Zdanie #922:\n",
      "   Tekst: Dziwne. ? Dlaczego tych pieniędzy nie włożył  do portfela który trzymał w ręku. Zwykły złodziej !!!\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "69. Zdanie #933:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account niezapomnij napisać w następnym tekście że każdy kiedyś umrze.W końcu to prawda\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "70. Zdanie #941:\n",
      "   Tekst: Prowokatorka HGW pokazała że Polskę i Polaków ma  głęboko w dupie.\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "71. Zdanie #963:\n",
      "   Tekst: @anonymized_account @anonymized_account A idźcie, najlepiej w pizdu.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "72. Zdanie #973:\n",
      "   Tekst: @anonymized_account @anonymized_account ale czy CHODZILIŚCIE a nie czy CHODZICIE\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "73. Zdanie #983:\n",
      "   Tekst: Jak się nazywa poseł, który bałamuci kobiety na miesiecznicach?\\nChwaliPięta.\n",
      "   Rzeczywista etykieta: 1\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "74. Zdanie #998:\n",
      "   Tekst: @anonymized_account @anonymized_account @anonymized_account kiedy on się dobrze zapowiadał ? Jak media pisały że go chce Chelsea ?\n",
      "   Rzeczywista etykieta: 0\n",
      "   Przewidywana etykieta: 1\n",
      "----------------------------------------\n",
      "\n",
      "Znaleziono 74 zdań zaklasyfikowanych jako klasa 1\n",
      "To stanowi 7.40% wszystkich zdań\n",
      "\n",
      "(Pokazuję tylko pierwsze 20 zdań)\n",
      "\n",
      "1. @anonymized_account bo wie już to kraina tą, kwa kwa agora zła i gęgę zły tvn\n",
      "\n",
      "2. @anonymized_account Jak gram na Orliku to nikt mi wynagrodzenia za to niedaje a biegam i staram się na całego wiec uważam ze to słabe wytłumaczenie\n",
      "\n",
      "3. @anonymized_account Miłego dnia Ci życzę 🌼\n",
      "\n",
      "4. RT @anonymized_account @anonymized_account Kiedy ten patologiczny kłamca skończy wreszcie opluwać Polaków i Polskę czy niema paragrafu na tę hienę?\n",
      "\n",
      "5. @anonymized_account Obronił wczoraj sam na sam.\n",
      "\n",
      "6. @anonymized_account Kolejna łże jak bura suka\n",
      "\n",
      "7. @anonymized_account @anonymized_account Zacząłeś robić za prymasa pajacu?\n",
      "\n",
      "8. @anonymized_account To w takim razie trzeba trzymać kciuki za spadek 😊\n",
      "\n",
      "9. @anonymized_account aż mi poprawił humor od rana hahahaha\n",
      "\n",
      "10. RT @anonymized_account Ej słuchajcie co zajebistego wymyśliłam \\n\\nJaki jest najlepszy argument bonnie z pamiętników wampirów? \\n\\nBo nnie\n",
      "\n",
      "11. RT @anonymized_account @anonymized_account Ale kamieniczkę chuj przytulił po miastowemu?\n",
      "\n",
      "12. Jutro po marszu, ale jeszcze kilka dni beda chodzić naćpani\n",
      "\n",
      "13. @anonymized_account @anonymized_account @anonymized_account odblokuj @anonymized_account w imię Jarkacza, szak to kawał jest kutasa\n",
      "\n",
      "14. Zbojkotować Mundial w tym dzikim kraju. To tak jak byśmy jechali grać w piłkę w tz. państwie islamskim.\n",
      "\n",
      "15. @anonymized_account Bardzo dobrze. Na piechotę pisfani. Wybraliście sobie PiS to macie.\n",
      "\n",
      "16. @anonymized_account I jeszcze jedno pytanie: onjaką wiarę chodzi w kontekście tych zabobonów?\n",
      "\n",
      "17. @anonymized_account Tak to jest z pomnikami obrońców pedofili\n",
      "\n",
      "18. @anonymized_account @anonymized_account A jaki Ty kurwa brzydki jesteś...\n",
      "\n",
      "19. Ani jednego człowieka z pis nie toleruję, tak mam, coś ze mną nie tak chyba\n",
      "\n",
      "20. @anonymized_account @anonymized_account \\\"Zadzwońcie do rady ministrów, że zaraz tam jadę\\\"\n"
     ]
    }
   ],
   "source": [
    "# Wypisz zdania z przewidywaną klasą 1\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ZDANIA ZAKLASYFIKOWANE JAKO KLASA 1:\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Znajdź indeksy gdzie przewidywania = 1\n",
    "predicted_class_1_indices = [i for i, pred in enumerate(all_predictions) if pred == 1]\n",
    "\n",
    "# Wypisz zdania\n",
    "for i, idx in enumerate(predicted_class_1_indices):\n",
    "    print(f\"{i+1}. Zdanie #{idx}:\")\n",
    "    print(f\"   Tekst: {X_val[idx]}\")\n",
    "    print(f\"   Rzeczywista etykieta: {all_labels[idx]}\")\n",
    "    print(f\"   Przewidywana etykieta: {all_predictions[idx]}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Podsumowanie\n",
    "print(f\"\\nZnaleziono {len(predicted_class_1_indices)} zdań zaklasyfikowanych jako klasa 1\")\n",
    "print(f\"To stanowi {len(predicted_class_1_indices)/len(all_predictions)*100:.2f}% wszystkich zdań\")\n",
    "\n",
    "# Opcjonalnie: wypisz tylko pierwsze N zdań jeśli jest ich dużo\n",
    "MAX_DISPLAY = 20\n",
    "if len(predicted_class_1_indices) > MAX_DISPLAY:\n",
    "    print(f\"\\n(Pokazuję tylko pierwsze {MAX_DISPLAY} zdań)\")\n",
    "    for i in range(MAX_DISPLAY):\n",
    "        idx = predicted_class_1_indices[i]\n",
    "        print(f\"\\n{i+1}. {X_val[idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
